<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>《Deep Learning》(2)-线性代数 | KangBing&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="介绍线性代数和矩阵的一些操作">
<meta property="og:type" content="article">
<meta property="og:title" content="《Deep Learning》(2)-线性代数">
<meta property="og:url" content="http://kangbing.github.io/2016/09/06/《Deep Learning》(2)-线性代数/index.html">
<meta property="og:site_name" content="KangBing's Blog">
<meta property="og:description" content="介绍线性代数和矩阵的一些操作">
<meta property="og:updated_time" content="2017-04-16T06:32:35.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="《Deep Learning》(2)-线性代数">
<meta name="twitter:description" content="介绍线性代数和矩阵的一些操作">
  
    <link rel="alternative" href="/atom.xml" title="KangBing&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
      <link rel="stylesheet" href="//cdn.bootcss.com/animate.css/3.5.0/animate.min.css" type="text/css">
  
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  <link rel="stylesheet" href="/font-awesome/css/font-awesome.min.css">
  <link rel="apple-touch-icon" href="/apple-touch-icon.png">
  
  
      <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  
  <script>
      var yiliaConfig = {
          fancybox: true,
          animate: true,
          isHome: false,
          isPost: true,
          isArchive: false,
          isTag: false,
          isCategory: false,
          open_in_new: false
      }
  </script>
  
      <script>
          var _hmt = _hmt || [];
          (function() {
              var hm = document.createElement("script");
              hm.src = "//hm.baidu.com/hm.js?1ad2f92496bbe7f551683e065f125883";
              var s = document.getElementsByTagName("script")[0]; 
              s.parentNode.insertBefore(hm, s);
          })();
      </script>
  
</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            
            <img lazy-src="/img/avatar.png" class="js-avatar">
            
        </a>

        <hgroup>
          <h1 class="header-author"><a href="/">KangBing</a></h1>
        </hgroup>

        
        <p class="header-subtitle">A Championship Heart!</p>
        
                


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">分类和标签</a></li>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <li id="Email"><a class="Email" target="_blank" href="mailto:kangyabing@126.com" title="Email"></a></li>
                            
                                <li id="GitHub"><a class="GitHub" target="_blank" href="/#" title="GitHub"></a></li>
                            
                                <li id="RSS"><a class="RSS" target="_blank" href="/atom.xml" title="RSS"></a></li>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <a href="/tags/AutoDiff/" style="font-size: 10px;">AutoDiff</a> <a href="/tags/BatchNorm/" style="font-size: 10px;">BatchNorm</a> <a href="/tags/DeepLearning/" style="font-size: 20px;">DeepLearning</a> <a href="/tags/Depthwise/" style="font-size: 10px;">Depthwise</a> <a href="/tags/Ensembles/" style="font-size: 10px;">Ensembles</a> <a href="/tags/Inception/" style="font-size: 12.5px;">Inception</a> <a href="/tags/Inception-V3/" style="font-size: 10px;">Inception-V3</a> <a href="/tags/Inception-V4/" style="font-size: 10px;">Inception-V4</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/ParameterServer/" style="font-size: 10px;">ParameterServer</a> <a href="/tags/Protocol-BUffers/" style="font-size: 10px;">Protocol BUffers</a> <a href="/tags/R-CNN/" style="font-size: 10px;">R-CNN</a> <a href="/tags/Residual/" style="font-size: 12.5px;">Residual</a> <a href="/tags/SGD/" style="font-size: 10px;">SGD</a> <a href="/tags/SPP-net/" style="font-size: 10px;">SPP-net</a> <a href="/tags/Xavier/" style="font-size: 10px;">Xavier</a> <a href="/tags/Xception/" style="font-size: 10px;">Xception</a> <a href="/tags/backforward/" style="font-size: 10px;">backforward</a> <a href="/tags/c/" style="font-size: 10px;">c++</a> <a href="/tags/cs231n/" style="font-size: 17.5px;">cs231n</a> <a href="/tags/feature/" style="font-size: 10px;">feature</a> <a href="/tags/glog/" style="font-size: 10px;">glog</a> <a href="/tags/google/" style="font-size: 12.5px;">google</a> <a href="/tags/kNN/" style="font-size: 10px;">kNN</a> <a href="/tags/python/" style="font-size: 10px;">python</a> <a href="/tags/softmax/" style="font-size: 10px;">softmax</a> <a href="/tags/数值计算/" style="font-size: 10px;">数值计算</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/概率/" style="font-size: 10px;">概率</a> <a href="/tags/神经网络/" style="font-size: 15px;">神经网络</a> <a href="/tags/线性代数/" style="font-size: 10px;">线性代数</a>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a target="_blank" class="main-nav-link switch-friends-link" href="https://hexo.io">Hexo</a>
                    
                      <a target="_blank" class="main-nav-link switch-friends-link" href="https://pages.github.com/">GitHub</a>
                    
                      <a target="_blank" class="main-nav-link switch-friends-link" href="http://moxfive.xyz/">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">专注写代码</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">KangBing</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                
                    <img lazy-src="/img/avatar.png" class="js-avatar">
                
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">KangBing</a></h1>
            </hgroup>
            
            <p class="header-subtitle">A Championship Heart!</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">分类和标签</a></li>
                
                    <li><a href="/about/">关于我</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <li id="Email"><a class="Email" target="_blank" href="mailto:kangyabing@126.com" title="Email"></a></li>
                            
                                <li id="GitHub"><a class="GitHub" target="_blank" href="/#" title="GitHub"></a></li>
                            
                                <li id="RSS"><a class="RSS" target="_blank" href="/atom.xml" title="RSS"></a></li>
                            
                        </ul>
            </nav>
        </header>                
    </div>
</nav>
      <div class="body-wrap"><article id="post-《Deep Learning》(2)-线性代数" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/09/06/《Deep Learning》(2)-线性代数/" class="article-date">
      <time datetime="2016-09-06T15:18:11.000Z" itemprop="datePublished">2016-09-06</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      《Deep Learning》(2)-线性代数
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/《Deep-Learning》笔记/">《Deep Learning》笔记</a>
    </div>


        
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DeepLearning/">DeepLearning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/线性代数/">线性代数</a></li></ul>
    </div>

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>介绍线性代数和矩阵的一些操作</p>
<ul>
<li><a id="more"></a>
</li>
</ul>
<h2 id="2-1_标量，向量，矩阵和张量">2.1 标量，向量，矩阵和张量</h2><ul>
<li>标量：只是一个数字。</li>
<li>向量：一串数字，相当于一维数组。一般默认向量都是列向量。</li>
<li>矩阵：一个2维的数组，需要用2个索引才可以确定其中一个元素。例如第$i$行，$j$列。</li>
<li>张量：维数大于2的数组，例如一个3维的张量<strong>A</strong>，需要坐标$(i,j,k)$才能确定其中一个元素。</li>
</ul>
<p>矩阵的转置：<br>
$$
(A^T)_{i,j}=A_{j,k}
$$
<br>向量只有一行/一列，向量转置可以看做只有一列的矩阵的转置。标量转置还是它自己。</p>
<p>矩阵加法，$A,B$都是矩阵<br>
$$
C = A + B
$$
<br>表示：<br>
$$
C_{i,j}=A_{i,j}+B_{i,j}
$$
<br>标量加或者乘以矩阵,$B$是矩阵，$a,c$是标量:<br>
$$
D=a \cdot B + c
$$
<br>表示:<br>
$$
D_{i,j}=a \cdot B_{i,j} + c
$$
<br>向量和矩阵相加，虽然看上去不那么合适，但是在深度学习中经常看到，$A$是矩阵，$b$是向量，那么：<br>$$<br>C=A+b<br>$$<br>表示：<br>
$$
C_{i,j}=A_{i,j} + b_j
$$
<br>即$A$的每一行都加上$b$。</p>
<h2 id="2-2_矩阵、向量的乘">2.2 矩阵、向量的乘</h2><p>$A$是$m \times n$的矩阵，$B$是$n \times p$的矩阵，那么<br>$$<br>C=AB<br>$$<br>$C$是$m \times p$的矩阵。其中<br>
$$
C_{i,j}=\sum_kA_{i,k}B_{k,j}
$$
<br>上面的这种矩阵乘法叫做element-wise product或Hadamard product，表示为$A \odot B$<br>还有一种矩阵乘法，叫做doc product，即点乘，这时两个矩阵维数相同，例如<br>$$<br>C=AB<br>$$<br>表示<br>
$$
C_{i,j}=A_{i,j}B_{i,j}
$$
<br>矩阵乘法满足一些乘法额定律：<br>分配律：<br>$$<br>A(B+C)=AB+AC<br>$$<br>结合律<br>$$<br>A(BC)=(AB)C<br>$$</p>
<p>不满足交换律，但是向量相乘满足交换律<br>$$<br>x^Ty=y^Tx<br>$$</p>
<p>矩阵乘积的转置有如下形式<br>$$<br>(AB)^T=B^TA^T<br>$$<br>除了上面的性质外，还应该知道一些特性：<br>线性等式:<br>$$<br>Ax=b<br>$$<br>其中$A \in R^{m \times n}$是矩阵，$b \in R^m$是向量，$A,b$都是已知的，$x \in R^n$是未知的。上面的矩阵乘法的等式相当于提供了表达等式的一种方式。</p>
<h2 id="2-3_矩阵求逆和单位矩阵">2.3 矩阵求逆和单位矩阵</h2><p>假设$I_n$是一个单位矩阵，$I_n \in R^{m \times n}$,那么<br>
$$
\forall x \in R^n, I_n x = x
$$
<br>单位矩阵，对角线上的数字都是1,其他部分数字全是0。</p>
<p>菊展$A$的逆表示为$A^{-1}$，有<br>
$$
A^{-1}A=I_n
$$
<br>那么2.2节中的等式<br>$$<br>Ax=b<br>$$<br>可以求解得到<br>
$$
x=A^{-1}b
$$
<br>当然，$A^{-1}$的存在是有条件的，不是每个矩阵都存在逆矩阵的。</p>
<p>当逆矩阵$A^{-1}$存在时，有不同的算法来闭式计算它。但是在计算机中，表示小数的精确度有限，因此有时并不能十分精确的得到$A^{-1}$，只能得到一个近似的估计值。</p>
<h2 id="2-4_线性依赖和跨度">2.4 线性依赖和跨度</h2><p>等式<br>$$<br>Ax=b<br>$$<br>如果$A^{-1}$，那么上面的等式对于不同的$b$必须只有一个解。但是也可能存在没有解或者有无数个解。对于特定具体的$b$,如果$x$和$y$都是解，那么下面也是一个解<br>
$$
z=\alpha x + (1-\alpha)y
$$
<br>也是一个解。其中$\alpha$是实数，那么这样就存在无数个解。</p>
<p>为了分析上面等式有多少个解，把$A$的每一列看做不同的方向，有多少条路可以到达$b$。这样，$x$的每个元素表示在对应的方向上走多远，$x_i$表示在第$i$列上移动多远：<br>
$$
Ax=\sum_i x_i A_{:,i}
$$
<br>上式这个操作叫做线性组合。</p>
<p>一个向量集合的生成空间是：这个向量集线性组合的所有点组成的空间。</p>
<p>$Ax=B$是否有解，在于$b$是否在$A$列向量集合的线性空间中。这个特殊的范围叫做列空间。</p>
<p>为了使$Ax=b$有解，$b \in R^m$，矩阵$A$的列空间应该包含$R^m$。矩阵有$n$列，即$n \geq m$。</p>
<p>$n \geq m$也只是必要条件，不是充分条件。因为有些列可能是冗余的；例如2列完全形同。这样的冗余叫做<em>线性依赖</em>。如果一个向量集线性独立，那么这个集合中的任何一个向量，都不能由其他向量的线性组合得到。这样，如果矩阵$A$的列空间包含$R^m$，那么它至少要有$m$个线性独立的列向量。这才是$Ax=b$有解的充分必要条件。</p>
<p>为了使矩阵$A$的逆存在，对于每一个$b$，等式$Ax=b$都应该至多有一个解。这样$A$最多有$m$列，否则可能会有多个解。</p>
<p>总和上面，矩阵$A$必须是个方阵，即$m=n$，且$A$的列线性独立。这样的矩阵叫做<strong>奇异矩阵</strong>。</p>
<p>只有$A$是奇异矩阵时才可以使用矩阵求逆的方法求解。</p>
<h2 id="2-5_范数">2.5 范数</h2><p>范数可以衡量向量的大小，一个向量的$L^p$范数定义如下：<br>
$$
||x||_p=\big(\sum_i |x_i|^p\big)^{\frac{1}{p}}
$$
<br>其中$p \in R, p \geq 1$</p>
<p>范数可以看做是向量到一个标量的映射函数，只要符合以下性质，都可以看做范数：<br>$f(x) = 0 \Rightarrow x=0$<br>$f(x+y) \leq f(x) + f(y)$（三角不等式）<br>$\forall \alpha \in R, f(\alpha x) = |\alpha|f(x)$</p>
<p>$L^2$范数是著名的欧几里得范数，它是原点到向量的欧拉距离。它可以省去下标，简写为$||x||$，$L^2$的平方形式等于$x^T x$。</p>
<p>$平方形式的L^2$范数在机器学习中用的最广，它的导数只和对应的项有关。但是平方形式的$L^2$范数在原点附近时，增长太慢。在一些机器学习的应用中，要十分严格区分0和非0的区别，这时可以使用$L^1$范数<br>
$$
||x||_1=\sum_i|x_i|
$$
<br>有时需要向量中非零元素的大小，有些人称作$L^0$范数，实际上没有$L^0$范数，可以使用$L^1$范数取代它。</p>
<p>在机器学习中经常用到$L^{\infty}$范数，称作最大范数（max norm），它的简化形式就是向量中最大数的幅度<br>
$$
||x||_{\infty}=\max_i|x_i|
$$
<br>在深度学习中，经常需要衡量矩阵的大小，这时使用Frobenius范数<br>
$$
||A||_F=\sqrt{\sum_{i,j}A_{i,j}^2}
$$
<br>和向量的$L^2$范数类似。</p>
<p>两个向量的点乘可以写作范数的形式<br>
$$
x^T y = ||x||_2 ||y||_2 \cos \theta
$$
<br>其中$\theta$是$x$和$y$的夹角。</p>
<h2 id="2-6特殊矩阵和向量">2.6特殊矩阵和向量</h2><p>对角矩阵：只有对角元素非零，其他元素都为零。$D$是对角元素，那么$D_{i,j}=0, i \neq j$。单位矩阵$I$就是对角矩阵。diag($v$)表示对角方形矩阵的对角元素组成的向量。计算和对角方阵的的乘积十分高效，例如计算diag($v$)$x$=$v \odot x$。对角元素不为零的对角方阵才存在逆矩阵，$diag(v)^{-1}=diag([1/v_1,…,1/v_n]^T)$</p>
<p>对角矩阵不一定是方形矩阵，但是只有方形对角矩阵才存在逆矩阵。非方形的对角矩阵相乘的代价也很小。</p>
<p>对称矩形是矩阵等于其转置<br>
$$
A = A^T
$$
<br>生成矩阵时，如果由2个变量的函数生成，且变量顺序无关，那么经常会生成对称矩阵，因为对称矩阵中$A<em>{i,j}=A</em>{j,i}$。</p>
<p>单位向量是指其范围为1<br>
$$
||v||_2=1
$$
<br>如果两个向量$x,y$正交，那么$x^Ty=0$；如果这两个向量的范数不为零，那么它们之间的角度为90度。在$R^n$中，至多有n个互相正交的非零向量。如果正交向量的范数为1，那么叫做标准正交。</p>
<p>正交矩阵的行和列都是正交的<br>
$$
A^TA=AA^T=I
$$
<br>即<br>
$$
A^{-1}=A^T
$$
</p>
<p>可以看出，计算正交矩阵的逆十分简单。</p>
<h2 id="2-7_特征分解">2.7 特征分解</h2><p>数学上的物体，可以分解。例如12可以分解为2x2x3；这样可以得知它不能被5整除，但是可以被3整除。同理，矩阵也可以分解。</p>
<p>用的最广的矩阵分解是特征向量和特征值。矩阵$A$的非零特征向量$v$，$A$乘以它可以得到缩放/放大的$v$<br>
$$
A v = \lambda v
$$
<br>其中，$\lambda$是对应的特征值。上式还有另一种形式$v^T A=\lambda v^T$。</p>
<p>如果$v$是特征向量，那么它缩放后依然是特征向量，所以我们只关心单位特征向量。</p>
<p>假设$A$有$n$个线性无关的特征向量${v^{(1)}, …, v^{(n)}}$，对应的特征值为${\lambda_1,…,\lambda_n}$，可以把特征向量当做一列来组成一个矩阵$V = [v^{(1)}, …, v^{(n)}]$，把特征值来组合成一个向量$\lambda = [\lambda_1,…,\lambda_n]$，$A$的特征分解为<br>
$$
A = V diag(\lambda)V^{-1}
$$
<br>可以通过特征值和特征向量来重建$A$。把矩阵分解为特征值和特征向量有助于我们分析理解矩阵。</p>
<p>不是每个矩阵都能分解为特征值和特征向量，有时分解还会产生复数。幸运的是，本书讲解的矩阵大部分都是可以简单分解的。注意一点，每个实数对称矩阵都可以分解，其特征值和特征向量都为实数：<br>
$$
A=Q \Lambda Q^T
$$
<br>其中$Q$是由$A$的特征向量组成的正交矩阵，$\Lambda$是对角矩阵。特征值$\Lambda_{i,i}$对应着矩阵$Q$的第$i$列$Q_{:,i}$</p>
<p>一个实对称矩阵$A$肯定有特征值分解，且特征值分解可能不唯一。如果两个或两个以上的特征向量有相等的特征值，由正交向量组成的集合和这个特征值也是矩阵的特征值分解。为了方便，我们常常将$\Lambda$降序排序；这样的话只有在特征值唯一时，特征分解才唯一。</p>
<p>矩阵的特征值分解给我们带来许多便利。例如，只有矩阵所有特征值为0时，它才是奇异矩阵。实数对称矩阵相乘可以用特征值分解优化，$f(x)=x^TAx$，且$||x||_2=1$；如果$x$是$A$的一个特征值，那么函数结果就是对应的特征向量；函数的最大值和最小值是对应特征值得最大值和最小值。</p>
<p>一个矩阵的所有特征值都大于0，那么就是正定的；如果都大于等于0，那么就是半正定的；如果都小于零，那么就是负定的；如果都小于等于零，那么就是半负定的。</p>
<h2 id="2-8奇异值分解">2.8奇异值分解</h2><p>上一节讲到了特征值分解，这一节来学奇异值分解（singular value decomposition, SVD）。像特征值分解一样，奇异值分解也可以帮助发现矩阵的一些特性；且奇异值分解更具一般性，每个矩阵都有奇异值分解。</p>
<p>特征分解形式为：<br>
$$
A= V diag(\lambda) V^{-1}
$$
<br>奇异值分解形式类似:<br>
$$
A= U D V^T
$$
<br>这里，假设$A$是$m \times n$的矩阵，那么$U$是$m \times m$的矩阵，$D$是$m \times n$的矩阵， $V$是$n \times n$的矩阵。且这三个矩阵有特殊的结构：$U$和$V$都是正交矩阵，$D$是对角矩阵，它不一定是方阵。</p>
<p>$D$叫做$A$的奇异值，$U$的列是$A$的左奇异向量(left-singular vector)，$V$的列是$A$的右奇异向量（right-singular vector）。</p>
<p>还可以通过简单的推导，得出奇异值分解和特征分解的关系。<br>
$$
A A^T=(U D V^T)(U D V^T)^T=U D V^T V D^T U^T=U D (V^T V) D^T U^T=U (DD^T) U^{-1}
$$
<br>因为$U$和$V$是正交矩阵：<br>
$$
UU^T=I, \qquad  VV^T=I
$$
<br>这样，可以把$U$当做$AA^T$的特征向量。同理,$V$是$A^TA$的特征向量。</p>
<p>奇异值分解的最有用的特性在于可以泛化，使非方阵可以部分求逆。</p>
<h2 id="2-9_广义伪逆矩阵">2.9 广义伪逆矩阵</h2><p>非方阵的逆矩阵没有定义。假设要对下面等式求解：<br>
$$
Ax=y
$$
<br>$B$是$A$左逆矩阵，等式左边乘以$B$可以得到<br>
$$
x=By
$$
<br>$A$到$B$的映射可能不唯一。$A$为$m \times n$，如果$m &gt;n $，可能没有解；如果$m &lt; n$可能有多个解。广义伪逆矩阵让我们可以无限接近，$A$的广义伪逆矩阵定义：<br>
$$
A^+ = \lim_{\alpha \searrow 0}(A^TA + \alpha I)^{-1} A^T
$$
<br>实际中使用的不是上面的形式，而是奇异值分解定义的形式：<br>
$$
A^+ = V D^+ D^T
$$
<br>$D^+$是把对角矩阵$D$伪逆矩阵，它是把用到了非零元素，之后由结果矩阵转置得到？？？(is obtained by taking the reciprocal of its non-zero element then taking the transpose of the resulting matrix)。</p>
<p>当$A$中,$m &lt; n$时，可以使用伪逆矩阵中的一个。尤其是经常用到的一个解为:$x=A^+ y$，其中$||x||_2$是所有解中最小的。</p>
<p>当$A$中，$m &gt; n$是可能没有解。这时使用伪逆转置，可以得到离$x$欧拉距离最近的解，即$$||Ax-y||_2$尽可能小。</p>
<h2 id="2-10迹算子">2.10迹算子</h2><p>迹算子可以得到矩阵对角线元素的和：<br>
$$
Tr(A)=\sum_i A_{i,j}
$$
<br>迹算子很重要，一些操作可以转换为求和操作，而求和操作需要用到矩阵相乘和迹算子。</p>
<p>例如Frobenius范数定义<br>
$$
||A||_F = \sqrt{Tr(AA^T)}
$$
<br>以迹算子形式定义一些操作，可以得到很多迹算子的性质。例如<br>
$$
Tr(A) = Tr(A^T)
$$
<br>如果是方阵，那么还有循环移动不变的性质<br>
$$
Tr(A B C) = TR(C B A) = Tr(B C A)
$$
<br>更一般的写法：<br>
$$
Tr(\prod_{i=1}^{n} F^{(i)}) = Tr(F^{(n)} \prod_{i=1}^{n-1} F^{(i)})
$$
<br>循环移动可能得到的矩阵大小都不同，例如$A \in R^{m \times n}$，$B \in R^{n \times m}$：<br>
$$
Tr(AB) = Tr(BA)
$$
<br>$AB \in R^{m \times m}$，$BA \in R^{n \times n}$。</p>
<p>对于标量:$a=Tr(a)$。</p>
<h2 id="2-11行列式">2.11行列式</h2><p>一个方矩阵的行列式由 det($A$)表示，它是一个把矩阵映射到实数的函数。一个矩阵的行列式等于其矩阵所有特征值的乘积。行列式的绝对值可以看做矩阵的在某个空间的体积。如果行列式等于零，那么这个空间至少全部收缩到了一维空间，是它体积为零。如果行列式等于1，那么变化过程中体积不变。</p>
<h2 id="2-12_例子：主成成分分析">2.12 例子：主成成分分析</h2><p>一个简单的机器学习算法叫做主成成分分析（principal components analysis, PAC），可以使用基本的线性代数来推导。</p>
<p>假设现在有$m$个点的集合${x^{(1)},…,x^{(m)}}$，所有点都在空间$R^n$上。我们打算对这些点实施有损压缩操作，有损压缩意味着占用空间会减少，但是会损失精度。我们想尽可能少的损失精度。</p>
<p>一种方法是把这些点编码到更低的维度空间去。可以找到一种编码方法，对于每个点$x^{(i)} \in R^n$，找到一个向量$c^{(i)} \in R^l$，如果$l &lt; n$，那么编码后的数据占用空间会减少。找到编码函数$f(x)=c$，解码函数可以重建$x$，使得$x \approx g(f(x))$</p>
<p>PCA就是要用到的解码函数，为了使解码简单，解码函数就是矩阵的相乘，映射回到$R^n$。$g(c)=Dc$，其中$D \in R^{n \times l}$，它是定义解码的矩阵。</p>
<p>计算最优的解码矩阵是个难题，为了使问题简单，PCA限制$D$的列互相正交。（注意$D$不是正交矩阵，除非$l=n$）。</p>
<p>这时解还不唯一，为了使解唯一，再加上限制：$D$的列有单位范数。</p>
<p>为了把想法转换为可以实现的算法，首先计算怎么为每个点$x$生成最优的$c^*$。一种方法是尽量减小$x$和它重建后点的距离，在主成分析中，使用$L^2$范数：<br>
$$
c^* = \arg_c \min ||x-g(c)||_2
$$
<br>转换为使用$L^2$的平方形式，因为它非负，且对于非负参数单调递增<br>
$$
c^* = \arg_c \min||x-g(c)||_2^2
$$
<br>即最小化<br>
$$
(x-g(c))^T(x-g(c))
$$
<br>由$L^2$定义展开<br>
$$
x^Tx - x^Tg(c) - g(c)^Tx + g(c)^Tg(c)
$$
<br>因为$x^Tg(c), g(c)^Tx$都是标量<br>
$$
x^Tx - 2x^Tg(c) + g(c)^T g(c)
$$
<br>因为只依赖变量$c$，可以忽略第一项<br>
$$
c^*=\arg_c \min -2x^T g(c) + g(c)^T g(c)
$$
<br>把g(c)的定义代入上式<br>
$$
c^*=\arg_c \min -2x^TDc + c^T D^T D c
$$
<br>即<br>
$$
=\arg_c \min -2x^TDc + c^T I_l c
$$
<br>因为$D$的列正交且有单位范数<br>
$$
=\arg_c \min-2x^T Dc + c^Tc
$$
<br>可以通过矢量微积分来求得上式的解：<br>
$$
\Delta_c(-2x^TDc + c^Tc)=0
$$
<br>即<br>
$$
-2D^Tx+2c=0
$$
<br>得到：<br>
$$
c=D^Tx
$$
<br>这样算法非常高效，对$x$编码，只需要矩阵乘法<br>
$$
f(x)=D^T x
$$
<br>再前面加上矩阵乘法，可以定义PCA重建矩阵操作<br>
$$
\gamma (x) = g(f(x))=DD^Tx
$$
<br>下一步要选择编码矩阵$D$。编码矩阵$D$是的输入矩阵和重建后的矩阵之间的$L^2$最小。编解码使用同一个矩阵，使得计算所有维度的误差矩阵的Frobenius范数最小：<br>
$$
D^*=\arg_D \min \sqrt{\sum_{i,j}(x_j^{(i)} - r(x^{(i)})_j)^2} \qquad subject \quad to \quad D^TD=I_l
$$
<br>先假设$l=1$，因为$\gamma (x)=D^TDx$，上式变为：<br>
$$
d^*=\arg_d \min_d \sum_i||x^{(i)} - dd^Tx^{(i)}||_2^2 \qquad subject \quad to \quad ||d||_2=1
$$
<br>上式是最直接的写法，但是在文体上更习惯把四叔放到左边，可以这样写<br>
$$
d^*=\arg_d \min_d \sum_i||x^{(i)} - d^Tx^{(i)}d||_2^2 \qquad subject \quad to \quad ||d||_2=1
$$
<br>或者把矩阵转置写到一起<br>
$$
d^*=\arg_d \min_d \sum_i||x^{(i)} - x^{(i)}dd^T||_2^2 \qquad subject \quad to \quad ||d||_2=1
$$
<br>把上面的向量写出矩阵形式，$X \in R^{m,n}$<br>
$$
d^* = \arg_d \min |X-Xdd^T||_F^2 \quad  subject \quad to \quad d^Td=1
$$
<br>忽略掉上面的约束项，简化Forbenius范式<br>
$$
\arg_d \min Tr( (X- Xdd^t)^T (X- Xdd^T) )
$$
<br>展开后为<br>
$$
\arg_d \min -Tr(X^TXdd^T) - Tr(dd^TX^TX) + Tr(dd^TX^TXdd^T)
$$
<br>和$d$无关的项不影响上面公式的结果<br>
$$
\arg_d \min -2Tr(X^TXdd^T) + Tr(dd^TX^TXdd^T)
$$
<br>对迹算子内擦矩阵进行循环移位<br>
$$
\arg_d \min - 2Tr(X^TXdd^T) + Tr(X^TXdd^Tdd^T)
$$
<br>有约束项$d^Td=1$得到<br>
$$
\arg_d \min -2Tr(X^TXdd^T) + Tr(X^TXdd^T) \quad subject \quad to \quad d^Td=1
$$
</p>

$$
\arg_d \min -Tr(X^TXdd^T)  \quad subject \quad to \quad d^Td=1
$$


$$
\arg_d \max Tr(X^TXdd^T)  \quad subject \quad to \quad d^Td=1
$$


$$
\arg_d \max Tr(d^TX^TXd)  \quad subject \quad to \quad d^Td=1
$$

<p>可以由特征值分解解决上面的最优化问题，即$d$是矩阵$X^TX$对应的最大的特征值。</p>
<p>在一般情况下，即$l &gt; 1$，矩阵$D$是$l$个最大特征值对应的特征向量。 </p>

      
    </div>
    
  </div>
  
    
    <div class="copyright">
        <p><span>本文标题:</span><a href="/2016/09/06/《Deep Learning》(2)-线性代数/">《Deep Learning》(2)-线性代数</a></p>
        <p><span>文章作者:</span><a href="/" title="访问 KangBing 的个人博客">KangBing</a></p>
        <p><span>发布时间:</span>2016年09月06日 - 23时18分</p>
        <p><span>最后更新:</span>2017年04月16日 - 14时32分</p>
        <p>
            <span>原始链接:</span><a class="post-url" href="/2016/09/06/《Deep Learning》(2)-线性代数/" title="《Deep Learning》(2)-线性代数">http://kangbing.github.io/2016/09/06/《Deep Learning》(2)-线性代数/</a>
            <span class="copy-path" data-clipboard-text="原文: http://kangbing.github.io/2016/09/06/《Deep Learning》(2)-线性代数/　　作者: KangBing" title="点击复制文章链接"><i class="fa fa-clipboard"></i></span>
            <script src="/js/clipboard.min.js"></script>
            <script> var clipboard = new Clipboard('.copy-path'); </script>
        </p>
        <p>
            <span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" title="中国大陆 (CC BY-NC-SA 3.0 CN)" target = "_blank">"署名-非商用-相同方式共享 3.0"</a> 转载请保留原文链接及作者。
        </p>
    </div>



    <nav id="article-nav">
        
            <div id="article-nav-newer" class="article-nav-title">
                <a href="/2016/09/06/《Deep Learning》(3)-概率和信息论/">
                    《Deep Learning》(3)-概率和信息论
                </a>
            </div>
        
        
            <div id="article-nav-older" class="article-nav-title">
                <a href="/2016/09/06/《Deep Learning》(1)-介绍/">
                    《Deep Learning》(1)-介绍
                </a>
            </div>
        
    </nav>

  
</article>

    <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1_标量，向量，矩阵和张量"><span class="toc-number">1.</span> <span class="toc-text">2.1 标量，向量，矩阵和张量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2_矩阵、向量的乘"><span class="toc-number">2.</span> <span class="toc-text">2.2 矩阵、向量的乘</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3_矩阵求逆和单位矩阵"><span class="toc-number">3.</span> <span class="toc-text">2.3 矩阵求逆和单位矩阵</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4_线性依赖和跨度"><span class="toc-number">4.</span> <span class="toc-text">2.4 线性依赖和跨度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5_范数"><span class="toc-number">5.</span> <span class="toc-text">2.5 范数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-6特殊矩阵和向量"><span class="toc-number">6.</span> <span class="toc-text">2.6特殊矩阵和向量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-7_特征分解"><span class="toc-number">7.</span> <span class="toc-text">2.7 特征分解</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-8奇异值分解"><span class="toc-number">8.</span> <span class="toc-text">2.8奇异值分解</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-9_广义伪逆矩阵"><span class="toc-number">9.</span> <span class="toc-text">2.9 广义伪逆矩阵</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-10迹算子"><span class="toc-number">10.</span> <span class="toc-text">2.10迹算子</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-11行列式"><span class="toc-number">11.</span> <span class="toc-text">2.11行列式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-12_例子：主成成分分析"><span class="toc-number">12.</span> <span class="toc-text">2.12 例子：主成成分分析</span></a></li></ol>
</div>
<style>
    .left-col .switch-btn {
        display: none;
    }
    .left-col .switch-area {
        display: none;
    }
</style>

<input type="button" id="tocButton" value="隐藏目录"  title="点击按钮隐藏或者显示文章目录">

<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js" type="text/javascript"></script>
<script>
    var valueHide = "隐藏目录";
    var valueShow = "显示目录";

    if ($(".left-col").is(":hidden")) {
        $("#tocButton").attr("value", valueShow);
    }

    $("#tocButton").click(function() {
        if ($("#toc").is(":hidden")) {
            $("#tocButton").attr("value", valueHide);
            $("#toc").slideDown(320);
            $(".switch-btn, .switch-area").fadeOut(300);
        }
        else {
            $("#tocButton").attr("value", valueShow);
            $("#toc").slideUp(350);
            $(".switch-btn, .switch-area").fadeIn(500);
        }
    })

    if ($(".toc").length < 1) {
        $("#toc, #tocButton").hide();
        $(".switch-btn, .switch-area").show();
    }
</script>




    <div class="share">
    <div class="bdsharebuttonbox">
    <a href="#" class="bds_more" data-cmd="more"></a>
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
    <a href="#" class="bds_sqq" data-cmd="sqq" title="分享给 QQ 好友"></a>
    <a href="#" class="bds_copy" data-cmd="copy" title="复制网址"></a>
    <a href="#" class="bds_mail" data-cmd="mail" title="通过邮件分享"></a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="生成文章二维码"></a>
    </div>
    <script>
        window._bd_share_config={
            "common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
    </script>
</div>



    
        <section class="youyan" id="comments">
  <div id="uyan_frame"></div>
  <script src="http://v2.uyan.cc/code/uyan.js?uid=2136539"></script>
</section>

    




    <div class="scroll" id="post-nav-button">
        
            <a href="/2016/09/06/《Deep Learning》(3)-概率和信息论/" title="上一篇: 《Deep Learning》(3)-概率和信息论">
                <i class="fa fa-angle-left"></i>
            </a>
        

        <a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a>

        
            <a href="/2016/09/06/《Deep Learning》(1)-介绍/" title="下一篇: 《Deep Learning》(1)-介绍">
                <i class="fa fa-angle-right"></i>
            </a>
        
    </div>

    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/11/04/MXNet/自动微分/">自动微分</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/25/MXNet/MXNet Data IO/">MXNet Data IO</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/14/MXNet/PS-Lite源码分析/">PS-Lite源码分析</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/26/Paper笔记/00010_Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition/">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/24/Paper笔记/0009_Rich feature hierarchies for accurate object detection and semantic segmentation/">Rich feature hierarchies for accurate object detection and semantic segmentation</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/13/Paper笔记/0008_A Neural Algorithm of Artistic Style/">A Neural Algorithm of Artistic Style</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/09/Paper笔记/0007_Xception Deep Learning with Depthwise Separable Convolutios/">Xception--Deep Learning with Depthwise Separable Convolutios</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/06/Paper笔记/0006_Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning/">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/04/Paper笔记/0005_Rethinking the Inception Architecture for Computer Vision/">Rethinking the Inception Architecture for Computer Vision</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/31/Paper笔记/0004_Residual Networks Behave Like Ensembles of Relatively Shallow Networks/">Residual Networks Behave Like Ensembles of Relatively Shallow Networks</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/14/Paper笔记/0003_Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift/">Batch Normalization--Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/11/Paper笔记/0002_Understanding the difficulty of training deep feedforward neural networks/">Understanding the difficulty of training deep feedforward neural networks</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/22/Paper笔记/0001_ILSVRC历届冠军论文笔记/">ILSVRC历届冠军论文笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/18/cs231n-(9)迁移学习和Fine-tune网络/">cs231n-(9)迁移学习和Fine-tune网络</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/18/cs231n-(8)理解和可视化卷积网络/">cs231n-(8)理解和可视化卷积网络</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/18/cs231n-(7)卷积神经网络：架构，卷积层,池化层/">cs231n-(7)卷积神经网络：架构，卷积层/池化层</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/18/cs231n-(6)实现Minimal神经网络/">cs231n-(6)实现Minimal神经网络</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/03/Python实现三层神经网络/">Python实现三层神经网络</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/22/cs231n-(5)神经网络-3：学习和评估/">cs231n-(5)神经网络-3：学习和评估</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/24/cs231n-(5)神经网络-2：设置数据和Loss/">cs231n-(5)神经网络-2：设置数据和Loss</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/06/《Deep Learning》(4)-数值计算/">《Deep Learning》(4)-数值计算</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/06/《Deep Learning》(3)-概率和信息论/">《Deep Learning》(3)-概率和信息论</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/06/《Deep Learning》(2)-线性代数/">《Deep Learning》(2)-线性代数</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/06/《Deep Learning》(1)-介绍/">《Deep Learning》(1)-介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/06/cs231n-(5)神经网络-1：建立架构/">cs231n-(5)神经网络-1：建立架构</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/04/cs231n-(4)反向传播/">cs231n-(4)反向传播</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/04/cs231n-(3)最优化：随机梯度下降/">cs231n-(3)最优化：随机梯度下降</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/04/cs231n-(2)线性分类器：SVM和Softmax/">cs231n-(2)线性分类器：SVM和Softmax</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/04/cs231n-(1)图像分类和kNN/">cs231n-(1)图像分类和kNN</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/03/21/机器学习(7)-神经网络预测练习/">机器学习(7)-神经网络预测练习</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/03/15/机器学习(6)-神经网络/">机器学习(6)-神经网络</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/03/09/机器学习(5)-逻辑回归练习/">机器学习(5)-逻辑回归练习</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/03/07/机器学习(4)-正则化/">机器学习(4)-正则化</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/03/06/机器学习(3)-逻辑回归/">机器学习(3)-逻辑回归</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/02/29/机器学习(2)-线性回归/">机器学习(2)-线性回归</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/02/27/机器学习(1)-概念/">机器学习(1)-概念</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/01/18/Google-Logging-Library-glog-使用/">Google Logging Library(glog)使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/01/10/获取目录下的所有子目录和文件/">获取目录下的所有子目录和文件</a></li><li class="post-list-item"><a class="post-list-link" href="/2015/12/21/Google Protocol BUffers使用/">Google Protocol BUffers使用</a></li></ul>
    <script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
    <script>
        $(".post-list").addClass("toc-article");
        $(".post-list-item a").attr("target","_blank");
        $("#post-nav-button > a:nth-child(2)").click(function() {
            $(".fa-bars, .fa-times").toggle();
            $(".post-list").toggle(300);
            if ($(".toc").length > 0) {
                $("#toc, #tocButton").toggle(200, function() {
                    if ($(".switch-area").is(":visible")) {
                        $("#toc, .switch-btn, .switch-area").toggle();
                        $("#tocButton").attr("value", valueHide);
                        }
                    })
            }
            else {
                $(".switch-btn, .switch-area").fadeToggle(300);
            }
        })
    </script>




    <script>
        
    </script>
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                &copy; 2018 KangBing
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的静态博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减双栏 Hexo 博客主题">Yelee</a> by MOxFIVE
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" >本站到访数: 
                            <span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>, </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit">本页阅读量: 
                            <span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
    </div>
    
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js" type="text/javascript"></script>
<script src="/js/main.js" type="text/javascript"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>




    <!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>
#add by kangyabing

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>

<script>
    $(document).ready(function() {
        if ($("#comments").length < 1) {
            $("#scroll > a:nth-child(2)").hide();
        };
    })
</script>


<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>

<!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
</body>
</html>