<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>《Deep Learning》(3)-概率和信息论 | KangBing&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="介绍线概率一些知识。概率论是表示不确定的数学基础。它提供了表示表示不确定的方法和求解不确定表达式的公理。在人工智能领域，概率论主要有两种用途。1、概率论告诉我们人工智能怎么推论，因此我们可以设计算法计算或近似由概率论推导出来的公式。2、可以使用概率论和统计在理论上分提出的AI系统的行为。">
<meta property="og:type" content="article">
<meta property="og:title" content="《Deep Learning》(3)-概率和信息论">
<meta property="og:url" content="http://kangbing.github.io/2016/09/06/《Deep Learning》(3)-概率和信息论/index.html">
<meta property="og:site_name" content="KangBing's Blog">
<meta property="og:description" content="介绍线概率一些知识。概率论是表示不确定的数学基础。它提供了表示表示不确定的方法和求解不确定表达式的公理。在人工智能领域，概率论主要有两种用途。1、概率论告诉我们人工智能怎么推论，因此我们可以设计算法计算或近似由概率论推导出来的公式。2、可以使用概率论和统计在理论上分提出的AI系统的行为。">
<meta property="og:image" content="http://7xras4.com1.z0.glb.clouddn.com/deeplearn03_01.jpg">
<meta property="og:image" content="http://7xras4.com1.z0.glb.clouddn.com/deeplearn03_02.jpg">
<meta property="og:updated_time" content="2016-09-15T15:45:13.349Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="《Deep Learning》(3)-概率和信息论">
<meta name="twitter:description" content="介绍线概率一些知识。概率论是表示不确定的数学基础。它提供了表示表示不确定的方法和求解不确定表达式的公理。在人工智能领域，概率论主要有两种用途。1、概率论告诉我们人工智能怎么推论，因此我们可以设计算法计算或近似由概率论推导出来的公式。2、可以使用概率论和统计在理论上分提出的AI系统的行为。">
  
    <link rel="alternative" href="/atom.xml" title="KangBing&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
      <link rel="stylesheet" href="//cdn.bootcss.com/animate.css/3.5.0/animate.min.css" type="text/css">
  
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  <link rel="stylesheet" href="/font-awesome/css/font-awesome.min.css">
  <link rel="apple-touch-icon" href="/apple-touch-icon.png">
  
  
      <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  
  <script>
      var yiliaConfig = {
          fancybox: true,
          animate: true,
          isHome: false,
          isPost: true,
          isArchive: false,
          isTag: false,
          isCategory: false,
          open_in_new: false
      }
  </script>
  
      <script>
          var _hmt = _hmt || [];
          (function() {
              var hm = document.createElement("script");
              hm.src = "//hm.baidu.com/hm.js?1ad2f92496bbe7f551683e065f125883";
              var s = document.getElementsByTagName("script")[0]; 
              s.parentNode.insertBefore(hm, s);
          })();
      </script>
  
</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            
            <img lazy-src="/img/avatar.png" class="js-avatar">
            
        </a>

        <hgroup>
          <h1 class="header-author"><a href="/">KangBing</a></h1>
        </hgroup>

        
        <p class="header-subtitle">A Championship Heart!</p>
        
                


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">分类和标签</a></li>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <li id="Email"><a class="Email" target="_blank" href="mailto:kangyabing@126.com" title="Email"></a></li>
                            
                                <li id="GitHub"><a class="GitHub" target="_blank" href="/#" title="GitHub"></a></li>
                            
                                <li id="RSS"><a class="RSS" target="_blank" href="/atom.xml" title="RSS"></a></li>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <a href="/tags/BatchNorm/" style="font-size: 10px;">BatchNorm</a> <a href="/tags/DeepLearning/" style="font-size: 20px;">DeepLearning</a> <a href="/tags/Depthwise/" style="font-size: 10px;">Depthwise</a> <a href="/tags/Ensembles/" style="font-size: 10px;">Ensembles</a> <a href="/tags/Inception/" style="font-size: 12.5px;">Inception</a> <a href="/tags/Inception-V3/" style="font-size: 10px;">Inception-V3</a> <a href="/tags/Inception-V4/" style="font-size: 10px;">Inception-V4</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/ParameterServer/" style="font-size: 10px;">ParameterServer</a> <a href="/tags/Protocol-BUffers/" style="font-size: 10px;">Protocol BUffers</a> <a href="/tags/R-CNN/" style="font-size: 10px;">R-CNN</a> <a href="/tags/Residual/" style="font-size: 12.5px;">Residual</a> <a href="/tags/SGD/" style="font-size: 10px;">SGD</a> <a href="/tags/SPP-net/" style="font-size: 10px;">SPP-net</a> <a href="/tags/Xavier/" style="font-size: 10px;">Xavier</a> <a href="/tags/Xception/" style="font-size: 10px;">Xception</a> <a href="/tags/backforward/" style="font-size: 10px;">backforward</a> <a href="/tags/c/" style="font-size: 10px;">c++</a> <a href="/tags/cs231n/" style="font-size: 17.5px;">cs231n</a> <a href="/tags/feature/" style="font-size: 10px;">feature</a> <a href="/tags/glog/" style="font-size: 10px;">glog</a> <a href="/tags/google/" style="font-size: 12.5px;">google</a> <a href="/tags/kNN/" style="font-size: 10px;">kNN</a> <a href="/tags/python/" style="font-size: 10px;">python</a> <a href="/tags/softmax/" style="font-size: 10px;">softmax</a> <a href="/tags/数值计算/" style="font-size: 10px;">数值计算</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/概率/" style="font-size: 10px;">概率</a> <a href="/tags/神经网络/" style="font-size: 15px;">神经网络</a> <a href="/tags/线性代数/" style="font-size: 10px;">线性代数</a>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a target="_blank" class="main-nav-link switch-friends-link" href="https://hexo.io">Hexo</a>
                    
                      <a target="_blank" class="main-nav-link switch-friends-link" href="https://pages.github.com/">GitHub</a>
                    
                      <a target="_blank" class="main-nav-link switch-friends-link" href="http://moxfive.xyz/">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">专注写代码</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">KangBing</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                
                    <img lazy-src="/img/avatar.png" class="js-avatar">
                
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">KangBing</a></h1>
            </hgroup>
            
            <p class="header-subtitle">A Championship Heart!</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">分类和标签</a></li>
                
                    <li><a href="/about/">关于我</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <li id="Email"><a class="Email" target="_blank" href="mailto:kangyabing@126.com" title="Email"></a></li>
                            
                                <li id="GitHub"><a class="GitHub" target="_blank" href="/#" title="GitHub"></a></li>
                            
                                <li id="RSS"><a class="RSS" target="_blank" href="/atom.xml" title="RSS"></a></li>
                            
                        </ul>
            </nav>
        </header>                
    </div>
</nav>
      <div class="body-wrap"><article id="post-《Deep Learning》(3)-概率和信息论" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/09/06/《Deep Learning》(3)-概率和信息论/" class="article-date">
      <time datetime="2016-09-06T15:18:30.000Z" itemprop="datePublished">2016-09-06</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      《Deep Learning》(3)-概率和信息论
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/《Deep-Learning》笔记/">《Deep Learning》笔记</a>
    </div>


        
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DeepLearning/">DeepLearning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/概率/">概率</a></li></ul>
    </div>

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>介绍线概率一些知识。概率论是表示不确定的数学基础。它提供了表示表示不确定的方法和求解不确定表达式的公理。在人工智能领域，概率论主要有两种用途。1、概率论告诉我们人工智能怎么推论，因此我们可以设计算法计算或近似由概率论推导出来的公式。2、可以使用概率论和统计在理论上分提出的AI系统的行为。</p>
<ul>
<li><a id="more"></a>
</li>
</ul>
<p>概率论是许多科学和工程的基础工具。这一节确保一些数学不扎实的软件工程师可以理解本书的数学。</p>
<h2 id="3-1_为什么需要概率？">3.1 为什么需要概率？</h2><p>计算机科学的许多分支处理的实体都是确定的。程序员可以安全的假设CPU将会完美无瑕地执行机器指令。硬件引起的问题太少了，以至于许多软件应用在设计时不用考虑它的发生。对比许多计算机工程师在相对稳定确定的环境下工作，机器学习使用概率论可能会让人惊讶。</p>
<p>机器学习处理的的事情是不确定的，有时还需要处理随机（非不确定）事情。而不确定性和随机性来自许多方面。总结一下，大概来自三个方面：<br>1、系统模型固有的随机性：例如，大部分量子论的解释，把原子内的微粒当做不确定的。例如洗牌，理论上我们假设了牌真正的随机洗过了。<br>2、不完整的观察：即使系统是确定的，但是我们也不能观察到所有影响系统行为的变量。<br>3、不完整的建模：当我们建模是，要舍弃一些信息。舍弃的信息导致模型预测的不确定性。</p>
<p>在许多实践中，更倾向于使用简单不确定的规则，也不去使用确定复杂的规则。例如，“鸟会飞，设计起来很简答”；但是真正正确的表述应该是“鸟当中，除了没有学会飞的幼鸟、生病的鸟、受伤的失去飞翔能力的鸟……，才会飞”。</p>
<p>概率论原本是描述事情发生的频率的。例如，在抽扑克游戏中，我们说一定概率$p$抽到某张牌，那么抽很多次，会大概有$p$比例的次数抽到这张牌；这是可以重复的实验。有些是不能重复的，例如一个医生说病人有40%的可能性患有流感，我们不能重复多次得到病人的拷贝来验证。这时需要<em>信度degree of belief</em>，1代表病人确定患有流感，0代表病人一定没有流感。<br>在上面两个例子中，第一种事件以一定概率发生，叫做<em>频率概率frequentist probability</em>。后一种，定性的准确性（例如诊断为流感情况下，诊断准确性的概率）叫做<em>贝叶斯概率Bayesian probability</em>。</p>
<p>如果要列出关于不确定性共有的特性，那么就是把贝叶斯概率和频率概率当做一样。例如，选手手中的牌已知，计算他赢得扑克游戏的概率；这和病人有某种症状，他患有某种病的概率计算方法相同。</p>
<p>概率论可以看做逻辑处理不确定性的拓展。在确定了命题A的真伪后，逻辑学为我们推导基于命题A的情况下，命题B的真伪；而概率论命题B真或伪可能性的大小。</p>
<h2 id="3-2随机变量">3.2随机变量</h2><p>随机变量是可以随机取一些值的变量。经常在变量右下角加上数字下标来表示随机变量可能的取值。例如，$x_1,x_2$是随机变量x可能取的值。如果是向量的话，<strong>x</strong>是随机变量，<strong>$x$</strong>是它可能取得值。</p>
<p>随机变量可能连续，可以能离散。离散随机变量状态有有限种，这些状态可以和数字无关。连续随机变量和一个实数相关联。</p>
<h2 id="3-3_概率分布">3.3 概率分布</h2><p>概率分布是用来描述变量怎么分布在各个状态的。描述变量分布的方式要取决于这个变量是离散，还是连续。</p>
<h3 id="3-3-1_离散变量和概率质量函数">3.3.1 离散变量和概率质量函数</h3><p>离散变量的概率分布用概率密度函数（probability mass function, PDF），经常用$P$表示。</p>
<p>概率质量函数把一个状态映射为这个状态出现的概率。例如$\textrm{x}=x$用$P(x)$表示；如果其值为1，表示一定是等于$x$，如果值为零，表示一定不等于$x$。$P(x)$可以这样写$P(\textrm x = x)$，或者$\textrm x \sim P(\textrm x)$</p>
<p>如果有多个变量，其联合分布$P(\textrm x = x, \textrm y = y)$表示$\textrm x = x, \textrm y = y$的概率，也常常简写为$P(x,y)$。</p>
<p>关于离散随机变量$x$的概率质量函数$P$满足一下性质：<br>1、$P$要覆盖$x$可能取值的所有状态。<br>2、$\forall x \in \textrm x, 0 \leq P(x) \leq 1$<br>3、$\sum_{x \in \textrm x} P(x) = 1$</p>
<h3 id="3-3-2_连续变量和概率密度函数">3.3.2 连续变量和概率密度函数</h3><p>连续变量的分布使用概率密度函数（Probability density function, PDF）来$p$表示，它满足<br>1、$p$必须覆盖变量$x$状态的所有范围<br>2、$\forall x \in \textrm x, 0 \leq p(x)$，注意并不要求$p(x) \leq 1$<br>3、$\int p(x)dx = 1$</p>
<p>概率密度函数并没有给出这个状态出现的概率，它乘以一个区间表示状态在这个区间的概率$p(x) \delta x$<br>例如在区间$[a, b]$的概率$\int_{[a,b]} p(x)dx$。</p>
<p>假设$x$在区间$[a,b]$上服从均匀分布，用函数$u(x;a,b)$表示。对于$x \notin [a,b]$，$u(x;a,b)=0$；对于$x \in [a,b]$,$u(x;a,b)=\frac{1}{b-a}$。这样的均匀分布，还可以用$x \sim U(a,b)$表示。</p>
<h2 id="3-4边际概率">3.4边际概率</h2><p>我们知道关于变量集合的概率分布，有时我们还想知道在这个变量集合子集合上的概率分布。这样的概率分布叫做边际概率分布（Marginal Probability）。</p>
<p>离散变量时，$P(\textrm x, \textrm y)$，可以使用求和准则得到<br>
$$
\forall x \in \textrm x, P(\textrm x = x) = \sum_y P(\textrm x = x, \textrm y = y)
$$
<br>可以把$P(\textrm x, \textrm y)$写成行和列的形式，那么求一行的和（或一列的和）就可以求得上式。</p>
<p>对于连续变量，使用积分代替求和<br>
$$
p(x) = \int p(x,y)dy
$$
</p>
<h2 id="3-5_条件概率">3.5 条件概率</h2><p>条件概率是在某事件已经发生情况下，另一个事件发生的概率。例如$\textrm x = x$已经发生时，$\textrm y = y$的概率为<br>
$$
P(\textrm y = y| \textrm x = x) = \frac{P(\textrm y =y, \textrm x = x)}{P(\textrm x = x)}
$$
<br>注意，上式中$P(\textrm x = x) &gt; 0$</p>
<h2 id="3-6_条件概率的链式法则">3.6 条件概率的链式法则</h2><p>联合概率函数，可以分解为只有一个变量的概率分布函数<br>
$$
P(\textrm x^{(1)},\dots, \textrm x^{(n)}) = P(\textrm x^{(1)}) \prod_{i=2}^n P(\textrm x^{(i)}|\textrm x^{(1)},\dots,x^{(i-1)})
$$
<br>可能看起来不太直观，直观一点为：<br>
$$
P(\textrm x^{(1)},\dots, \textrm x^{(n)})=P(\textrm x^{(1)}) P(\textrm x^{(2)}|\textrm x^{(1)}) P(\textrm x^{(3)}|\textrm x^{(1)} \textrm x^{(2)}) \dots
$$
<br>这是条件概率的链式法则。将上面定义应用两次<br>
$$
P(a,b,c) = P(a|b,c) P(b,c)
$$
<br>
$$
P(b,c) = P(b|c) P(c)
$$
</p>

$$
P(a,b,c)= P(a|b,c) P(b|c) P(c)
$$

<h2 id="3-7独立和条件独立">3.7独立和条件独立</h2><p>如果两个变量独立，那么它们的联合概率等于它们概率的乘积。即$x,y$独立<br>
$$
\forall x \in \textrm x, y \in \textrm y, p(\textrm x = x, \textrm y = y)=p(\textrm x = x)p(\textrm y = y)
$$
<br>可以用$\textrm x \perp \textrm y$表示。</p>
<p>$x,y$在给定$z$是条件独立<br>
$$
\forall x \in \textrm x, y \in \textrm y, z \in textrm z, p(\textrm x = x, \textrm y = y|\textrm z = z)=p(\textrm x = x|\textrm z = z)p(\textrm y = y|\textrm z = z)
$$
<br>可以用$\textrm x \perp \textrm y|\textrm z$表示。</p>
<h2 id="3-8_期望，方差和协方差">3.8 期望，方差和协方差</h2><p>函数$f(x)$关于概率分布$P(\textrm x)$的期望可以用求和或积分求得：<br>
$$
E_{x \sim P}[f(x)]=\sum_x P(x)f(x)
$$
<br>或<br>
$$
E_{x \sim P}[f(x)]=\int P(x)f(x)dx
$$
</p>
<p>期望是线性运算，例如<br>
$$
E_x[\alpha f(x) + \beta g(x)] = \alpha E_x[f(x)] + \beta E_x [g(x)]
$$
<br>其中$\alpha, \beta$不依赖$x$</p>
<p>方差用来描述变量的波动大小的，定义如下：<br>
$$
Var(f(x)) = E[(f(x) - E[f(x)])^2]
$$
<br>如果方差比较小，说明$f(x)$聚集在其期望附近。方差的平方根叫做标准差。</p>
<p>协方差用来描述两个变量的线性依赖关系的强弱，定义如下<br>
$$
Cov(f(x),g(x)) = E[(f(x)-E[f(x)])(g(y)-E[g(y)])]
$$
<br>如果协方差绝对值比较大，说明两个变量同时距离均值比较远。如果取值为正，说明两者同时变大；如果为负，说明两者一个变大，另外一个变小。其他衡量方法，例如相关系数，是把分布标准化，用来衡量它们之间相关性的大小。</p>
<p>协方相关和依赖有关系，但是它们是不同的概念。有关系，是因为两个独立的变量的方差为零；如果两个变量的协方差不为零，那么它们有依赖。独立和协相关是两个不同的属性。如果两个变量协方差为零，那么它们一定没有线性依赖关系。独立的要求更高，因为独立不仅仅要求非线性相关；零协方差只表示非线性相关。</p>
<p>例如从在区间$[-1,1]$上均匀分布上去一点$x$，在集合$(-1,1)$中取一个数$s$。假设$y=sx$，$s$决定符号，而$x$决定幅度。显然$x,y$相关，但是$Cov(x,y)=0$。</p>
<p>向量$x \in R^n$的协方差矩阵是一个$n \times n$的矩阵<br>
$$
Cov(\mathbf x)_{i,j} = Cov(x_i,x_j)
$$
<br>协方差矩阵的对角就是方差<br>
$$
Con(x_i,x_i)=Var(x_i)
$$
</p>
<h2 id="3-9常用概率分布">3.9常用概率分布</h2><p>介绍几个常见的概率分布</p>
<h3 id="伯努利分布">伯努利分布</h3><p>伯努利分布式一个二项分布，它只有一个变量表示等于1的概率：$\phi \in [0,1]$<br>
$$P(\textrm x = 1) = \phi$$
<br>
$$P(\textrm x = 0) = 1-\phi$$
<br>综合一下为：<br>
$$P(\textrm x = x) = \phi^x(1-\phi)^{1-x} $$
<br>期望和方差为：<br>
$$E_{\textrm x}[\textrm x] = \phi$$
$$Var_{\textrm x}(\textrm x) = \phi(1-\phi)$$
</p>
<h3 id="多项分布">多项分布</h3><p>伯努利分布只有2个状态，多项分布状态可以大于2个。<br>伯努利分布和二项分布在离散变量分布中常常用到，因为离散变量状态可以统计。连续变量状态时，上面两个分布就不适用了。</p>
<h3 id="高斯分布">高斯分布</h3><p>高斯分布也叫作标准分布：<br>
$$
\mathcal N(x;\mu, \sigma^2)=\sqrt{\frac{1}{2 \pi \sigma^2}}\exp(-\frac{1}{2\sigma^2}(x-\mu)^2)
$$
<br>分布有两个参数$\mu \in R$和$\sigma \in (0, \infty)$控制，前者是均值，后者是方差:$E(x)=\mu, Var(x)=\sigma^2$.</p>
<p>还有一种形式<br>
$$
\mathcal N(x;\mu, \beta)=\sqrt{\frac{\beta}{2 \pi}}\exp(-\frac{1}{2}\beta(x-\mu)^2)
$$
<br>在应用中常常使用高斯分布。在缺少先验知识情况下，使用高斯分布是一个明智的选择。因为：<br>1、我们要估计的分布可能就接近高斯分布。<br>2、在方差大小相同情况下，高斯分布包含的不确定性最大（即信息量最大）。</p>
<p>上面是单变量的高斯分布，把它扩展到多维叫做多方差标准分布，要用到正定对称矩阵$\Sigma$<br>
$$
\mathcal N(x;\mu, \Sigma)=\sqrt{\frac{1}{(2 \pi)^n det(\Sigma)}}\exp(-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu))
$$
<br>$\mu$是分布的均值，这时是个矩阵。$\Sigma$是分布的协方差矩阵。还可以写成<br>
$$
\mathcal N(x;\mu, \beta^{-1})=\sqrt{\frac{det(\beta)}{(2 \pi)^n }}\exp(-\frac{1}{2}(x-\mu)^T \beta(x-\mu))
$$
<br>经常把协方差矩阵变为对角矩阵。还有一个更简单的isotropic高斯分布，它的协方差矩阵为单位矩阵乘以一个标量。</p>
<h3 id="指数和拉普拉斯分布">指数和拉普拉斯分布</h3><p>在深度学习中，我们经常想要一个在$x=0$处有尖点（sharp point）的概率分布，指数分布（exponential distribution）就能满足这一点<br>
$$
p(x; \lambda)=\lambda \textbf 1_{x \geq 0} \exp(-\lambda x)
$$
<br>其中$1_{x \geq 0}$表示当$x$为负数时，概率为零。</p>
<p>一个近似相关的拉普拉斯分布（Laplace distribution）可以让我们在点$\mu$有锐点<br>
$$
\text{Laplace}(x;\mu,\gamma)=\frac{1}{2\gamma} \exp (-\frac{|x-\mu|}{\gamma})
$$
</p>
<h3 id="狄拉克分布和经验分布">狄拉克分布和经验分布</h3><p>在一些实例中，我们希望把概率分布的的所有质量（mass）都聚集到一个点，这时可以使用狄拉克分布$\delta(x)$<br>
$$
p(x)=\delta(x-\mu)
$$
<br>$\delta(x)$在非零点，其值为0，但是它积分还是1。狄拉克分布不是普通的函数，它是泛化函数（generalized function）。可以这样认为：狄拉克函数把其他地方所有的质量都一点点集中到了0处。它在$x=0$时值无限大，因为积分为1。</p>
<p>还有一个更常用的有狄拉克组成的分布，叫做经验分布<br>
$$
\hat p(x)=\frac{1}{m}\sum_{i=1}^{m}\delta(x-x^{(i)})
$$
<br>狄拉克分布是定义在连续变量上的。</p>
<p>我们可以把狄拉克分布看做，从训练集中采样一些样本，使用采样的样本训练训练模型。</p>
<h3 id="混合分布">混合分布</h3><p>常常联合几个概率分布来定义新的概率分布。经验分布就是狄拉克分布组合而来。</p>
<p>在使用联合混合分布时，那个分布起作用可以用多项分布控制<br>
$$
P(x) = \sum_i P(c = i)P(x|c=i)
$$
<br>其中$P(c)$就是一个多项分布。</p>
<p>混合模型中，可以引出一个概念：潜在变量（latent variable）。潜在变量使我们不能直接观察到的变量，在上面的混合模型中$c$就是一个例子。潜在变量通过联合概率分布和$x$产生联系$P(x,c)=P(x|c)P(c)$，分布$P(c)$并不能直接观察到，但是我们还是可以定义$P(x)$</p>
<p>非常重要和常用的联合模型是高斯混合模型，其中$p(x|c=i)$是高斯的。每个组成部分有单独的均值$\mu^{(i)}$和方差$\Sigma^{(i)}$；在一些混合模型中，可能有对变量有更多限制。</p>
<p>除了均值和方差，高斯混合分布指定了每个$i$的先验分布（prior probability）$\alpha_i = P(c=i)$。先验是指在观察到$x$以前已经知道$c$。一个对比，$P(c|x)$是后验概率，因为它在观察到$x$后才计算。高斯混合模型是常用的近似密度，因为任何平滑的密度都可以被多变量高斯混合模型近似。</p>
<h2 id="3-10常用函数的有用特性">3.10常用函数的有用特性</h2><p>logistic sigmoid<br>
$$
\sigma (x) = \frac{1}{1 + \exp(-x)}
$$
<br>常常用来生成伯努利分布，因为它的输出范围是$(0,1)$。</p>
<p>softplus<br>
$$
\zeta(x) = log(1 + \exp(x))
$$
<br>softpuls常常为标准分布生成$\beta$或$\sigma$，因为它的输出范围是$(0, \infty)$</p>
<p>softpuls使用$max(0,x)$变化而来的，是它的平滑版本。</p>
<p>下面性质很有用，希望你能记住<br>
$$
\sigma(x) = \frac{\exp(x)}{\exp(x) + \exp(0)} \\\
\frac{d}{dx}= \sigma(x)(1-\sigma(x)) \\\
1-\sigma(x) = \sigma(-x) \\\
\log(\sigma(x) = -\zeta(-x) \\\
\frac{d}{dx}\zeta(x) = \sigma(x) \\\
\forall x \in (0,1), \sigma^{-1}(x)=\log\frac{x}{1-x} \\\
\forall x > 0, \zeta^{-1}(x)=\log(\exp(x) - 1) \\\
\zeta(x)=int_{-\infty}^{x}\sigma(y)dy \\\
\zeta(x) - \zeta(-x)=x
$$
</p>
<h2 id="3-11贝叶斯准则">3.11贝叶斯准则</h2><p>已知$P(y|x)$，想知道$P(x|y)$；如果知道$P(x)$，可以使用贝叶斯准则计算<br>
$$
P(x|y)=\frac{P(x)P(y|x)}{P(y)}
$$
<br>$P(y)$可以通过$P(y)=\sum_{x}P(y|x)P(x)$计算得来。<br>贝叶斯准则使用计算条件概率的。</p>
<h2 id="3-12连续变量的一些技术细节">3.12连续变量的一些技术细节</h2><p>对于两个连续变量$x,y$，有如下关系$y=g(x)$，这里$g$是连续、可逆、可谓分的变换。现在来找$p_y(y)$和$p<em>x(x)$的关系。<br>
$$
|p_y(g(x))dy|=|p_x(x)dx|
$$
<br>可以得到<br>
$$
p_y(y)=p_x(g^{-1}(y))\frac{\partial x}{\partial y}
$$

另一种形式
{% raw %}
$$
p_x(x)=p_y(g(x))\frac{\partial g(x)}{\partial x}
$$
<br>在高维空间中，微分泛化为雅克比矩阵的行列式$J</em>{i,j}=\frac{\partial x_i}{\partial y_j}$<br>
$$
p_x(x)p_y(g(x))|\det (\frac{\partial g(x)}{\partial x})|
$$
</p>
<h2 id="3-13信息论">3.13信息论</h2><p>衡量一个事件的信息量，应该有一下准则：<br>1、发生概率越大的事件包含信息量越小。<br>2、发生可能性越小的事件，包含信息量越大。<br>3、相互独立的事件，信息量可以相加</p>
<p>定义自信息（self-information）,$\textrm x = x$<br>
$$
I(x)=-\log P(x)
$$
<br>自信息只是定义单个事件，衡量一个概率分布的信息量使用香农熵（Shannon entropy）<br>
$$
H(x) = E_{x \sim P}[I(x)] = -E_{x \sim P}[\log P(x)]
$$
<br>有两个关于$x$的分布$P(x)$、$Q(x)$，衡量两个分布的不同，可以使用相对熵（Kullback-Leibler divergence）<br>
$$
D_{KL}(P||Q)=E_{x \sim p}[\log \frac{P(x)}{Q(x)}]=E_{x \sim p}[\log P(x) - \log Q(x)]
$$
<br>在机器学习中，常常这样使用：$P$是真实分布，从中抽取一些符号，用来估计分布得到$Q$，要做的就是最小化$D_{KL}$。</p>
<p>$D<em>{KL}$有许多有用的特性，用的最多的就是非负性。它用来衡量两个分布的距离，用一个分布估计另一个分布，最小化它们之间的$D</em>{KL}$即可。注意，$D<em>{KL}$不是非负的。$D</em>{KL}(P||Q) \neq D_{KL}(Q||P)$，在使用时要注意用哪个。</p>
<p>它和交叉熵相关，交叉熵为$H(P,Q) = H(P) + D_{KL}(P||Q)$，缺少左边部分，变为：<br>
$$
H(P,Q) = -E_{x \sim P} \log Q(x)
$$
<br>最小化和$Q$相关的交叉熵等价于最小化KL距离，因为$Q$和$H(P)$无关，忽略它。</p>
<h2 id="3-14构造概率模型">3.14构造概率模型</h2><p>机器学习中的概率分布经常和许多变量相关。但是这些概率分布常常只和几个变量直接相关。使用单一函数构造概率分布效率低下，这时可以把概率分布划分为几个相关因子，之后再相乘。例如有三个变量$a,b,c$，$a$影响$b$，$b$影响$c$，但是在给定$b$时$a,c$不相关。可以这样描述这个分布<br>
$$
p(a,b,c) = p(a)p(b|a)p(c|b)
$$
<br>这个因式分解可以极大减少描述分布的参数。</p>
<p>可以用图来描述这样的因式分解：顶点的集合通过边来互相连接。当用图来表示概率的因式分解时，叫做构造概率模型后图模型。</p>
<p>主要有两种类型的构造概率模型：有向模型的和无向模型。两种类型都是使用图，顶点表示一个变量，通过边相关联的两个变量表示这两个变量在概率分布中有直接关系。</p>
<p>有向模型：图中的边是有向。如下图<br><img src="http://7xras4.com1.z0.glb.clouddn.com/deeplearn03_01.jpg" alt="deeplearing03_01.jpg"><br>关联的顶点的概率和它的父节点变量相关，父节点定义为$Pa_{\mathcal G}(x_i)$<br>
$$
p(x) = \prod_i p(x_i|Pa_{\mathcal G}(x_i))
$$
<br>无向模型使用无向表示，它表示因式分解时使用一系列函数；这些函数和有向模型不同，它们不是任何形式的概率分布。几个顶点的集合叫做圈（clique），一个圈在一用变量$\phi^{(i)}(C^{(i)})$表示，它表示函数而不是分布。每个函数的输出大于0，但是并不保证其积分等于1。可以除以$Z$归一化，归一化后的概率分布为：<br>
$$
p(x) = \frac{1}{Z}\prod_i \phi^{(i)}(C^{(i)})
$$
<br>如下图<br><img src="http://7xras4.com1.z0.glb.clouddn.com/deeplearn03_02.jpg" alt="deeplearning03_03.jpeg"><br>概率分布为：<br>
$$
p(a,b,c,d,e)=\frac{1}{Z}\phi^{(1)}(a,b,c)\phi^{(2)}(b,d)\phi^{(3)}(c,e)
$$
</p>

      
    </div>
    
  </div>
  
    
    <div class="copyright">
        <p><span>本文标题:</span><a href="/2016/09/06/《Deep Learning》(3)-概率和信息论/">《Deep Learning》(3)-概率和信息论</a></p>
        <p><span>文章作者:</span><a href="/" title="访问 KangBing 的个人博客">KangBing</a></p>
        <p><span>发布时间:</span>2016年09月06日 - 23时18分</p>
        <p><span>最后更新:</span>2016年09月15日 - 23时45分</p>
        <p>
            <span>原始链接:</span><a class="post-url" href="/2016/09/06/《Deep Learning》(3)-概率和信息论/" title="《Deep Learning》(3)-概率和信息论">http://kangbing.github.io/2016/09/06/《Deep Learning》(3)-概率和信息论/</a>
            <span class="copy-path" data-clipboard-text="原文: http://kangbing.github.io/2016/09/06/《Deep Learning》(3)-概率和信息论/　　作者: KangBing" title="点击复制文章链接"><i class="fa fa-clipboard"></i></span>
            <script src="/js/clipboard.min.js"></script>
            <script> var clipboard = new Clipboard('.copy-path'); </script>
        </p>
        <p>
            <span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" title="中国大陆 (CC BY-NC-SA 3.0 CN)" target = "_blank">"署名-非商用-相同方式共享 3.0"</a> 转载请保留原文链接及作者。
        </p>
    </div>



    <nav id="article-nav">
        
            <div id="article-nav-newer" class="article-nav-title">
                <a href="/2016/09/06/《Deep Learning》(4)-数值计算/">
                    《Deep Learning》(4)-数值计算
                </a>
            </div>
        
        
            <div id="article-nav-older" class="article-nav-title">
                <a href="/2016/09/06/《Deep Learning》(2)-线性代数/">
                    《Deep Learning》(2)-线性代数
                </a>
            </div>
        
    </nav>

  
</article>

    <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1_为什么需要概率？"><span class="toc-number">1.</span> <span class="toc-text">3.1 为什么需要概率？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2随机变量"><span class="toc-number">2.</span> <span class="toc-text">3.2随机变量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3_概率分布"><span class="toc-number">3.</span> <span class="toc-text">3.3 概率分布</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-1_离散变量和概率质量函数"><span class="toc-number">3.1.</span> <span class="toc-text">3.3.1 离散变量和概率质量函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-2_连续变量和概率密度函数"><span class="toc-number">3.2.</span> <span class="toc-text">3.3.2 连续变量和概率密度函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4边际概率"><span class="toc-number">4.</span> <span class="toc-text">3.4边际概率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-5_条件概率"><span class="toc-number">5.</span> <span class="toc-text">3.5 条件概率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-6_条件概率的链式法则"><span class="toc-number">6.</span> <span class="toc-text">3.6 条件概率的链式法则</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-7独立和条件独立"><span class="toc-number">7.</span> <span class="toc-text">3.7独立和条件独立</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-8_期望，方差和协方差"><span class="toc-number">8.</span> <span class="toc-text">3.8 期望，方差和协方差</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-9常用概率分布"><span class="toc-number">9.</span> <span class="toc-text">3.9常用概率分布</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#伯努利分布"><span class="toc-number">9.1.</span> <span class="toc-text">伯努利分布</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#多项分布"><span class="toc-number">9.2.</span> <span class="toc-text">多项分布</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#高斯分布"><span class="toc-number">9.3.</span> <span class="toc-text">高斯分布</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#指数和拉普拉斯分布"><span class="toc-number">9.4.</span> <span class="toc-text">指数和拉普拉斯分布</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#狄拉克分布和经验分布"><span class="toc-number">9.5.</span> <span class="toc-text">狄拉克分布和经验分布</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#混合分布"><span class="toc-number">9.6.</span> <span class="toc-text">混合分布</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-10常用函数的有用特性"><span class="toc-number">10.</span> <span class="toc-text">3.10常用函数的有用特性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-11贝叶斯准则"><span class="toc-number">11.</span> <span class="toc-text">3.11贝叶斯准则</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-12连续变量的一些技术细节"><span class="toc-number">12.</span> <span class="toc-text">3.12连续变量的一些技术细节</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-13信息论"><span class="toc-number">13.</span> <span class="toc-text">3.13信息论</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-14构造概率模型"><span class="toc-number">14.</span> <span class="toc-text">3.14构造概率模型</span></a></li></ol>
</div>
<style>
    .left-col .switch-btn {
        display: none;
    }
    .left-col .switch-area {
        display: none;
    }
</style>

<input type="button" id="tocButton" value="隐藏目录"  title="点击按钮隐藏或者显示文章目录">

<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js" type="text/javascript"></script>
<script>
    var valueHide = "隐藏目录";
    var valueShow = "显示目录";

    if ($(".left-col").is(":hidden")) {
        $("#tocButton").attr("value", valueShow);
    }

    $("#tocButton").click(function() {
        if ($("#toc").is(":hidden")) {
            $("#tocButton").attr("value", valueHide);
            $("#toc").slideDown(320);
            $(".switch-btn, .switch-area").fadeOut(300);
        }
        else {
            $("#tocButton").attr("value", valueShow);
            $("#toc").slideUp(350);
            $(".switch-btn, .switch-area").fadeIn(500);
        }
    })

    if ($(".toc").length < 1) {
        $("#toc, #tocButton").hide();
        $(".switch-btn, .switch-area").show();
    }
</script>




    <div class="share">
    <div class="bdsharebuttonbox">
    <a href="#" class="bds_more" data-cmd="more"></a>
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
    <a href="#" class="bds_sqq" data-cmd="sqq" title="分享给 QQ 好友"></a>
    <a href="#" class="bds_copy" data-cmd="copy" title="复制网址"></a>
    <a href="#" class="bds_mail" data-cmd="mail" title="通过邮件分享"></a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="生成文章二维码"></a>
    </div>
    <script>
        window._bd_share_config={
            "common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
    </script>
</div>



    
        <section class="youyan" id="comments">
  <div id="uyan_frame"></div>
  <script src="http://v2.uyan.cc/code/uyan.js?uid=2136539"></script>
</section>

    




    <div class="scroll" id="post-nav-button">
        
            <a href="/2016/09/06/《Deep Learning》(4)-数值计算/" title="上一篇: 《Deep Learning》(4)-数值计算">
                <i class="fa fa-angle-left"></i>
            </a>
        

        <a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a>

        
            <a href="/2016/09/06/《Deep Learning》(2)-线性代数/" title="下一篇: 《Deep Learning》(2)-线性代数">
                <i class="fa fa-angle-right"></i>
            </a>
        
    </div>

    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/06/25/MXNet/MXNet Data IO/">MXNet Data IO</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/14/MXNet/PS-Lite源码分析/">PS-Lite源码分析</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/26/Paper笔记/00010_Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition/">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/24/Paper笔记/0009_Rich feature hierarchies for accurate object detection and semantic segmentation/">Rich feature hierarchies for accurate object detection and semantic segmentation</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/13/Paper笔记/0008_A Neural Algorithm of Artistic Style/">A Neural Algorithm of Artistic Style</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/09/Paper笔记/0007_Xception Deep Learning with Depthwise Separable Convolutios/">Xception--Deep Learning with Depthwise Separable Convolutios</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/06/Paper笔记/0006_Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning/">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/04/Paper笔记/0005_Rethinking the Inception Architecture for Computer Vision/">Rethinking the Inception Architecture for Computer Vision</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/31/Paper笔记/0004_Residual Networks Behave Like Ensembles of Relatively Shallow Networks/">Residual Networks Behave Like Ensembles of Relatively Shallow Networks</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/14/Paper笔记/0003_Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift/">Batch Normalization--Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/11/Paper笔记/0002_Understanding the difficulty of training deep feedforward neural networks/">Understanding the difficulty of training deep feedforward neural networks</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/22/Paper笔记/0001_ILSVRC历届冠军论文笔记/">ILSVRC历届冠军论文笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/18/cs231n-(9)迁移学习和Fine-tune网络/">cs231n-(9)迁移学习和Fine-tune网络</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/18/cs231n-(8)理解和可视化卷积网络/">cs231n-(8)理解和可视化卷积网络</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/18/cs231n-(7)卷积神经网络：架构，卷积层,池化层/">cs231n-(7)卷积神经网络：架构，卷积层/池化层</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/18/cs231n-(6)实现Minimal神经网络/">cs231n-(6)实现Minimal神经网络</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/03/Python实现三层神经网络/">Python实现三层神经网络</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/22/cs231n-(5)神经网络-3：学习和评估/">cs231n-(5)神经网络-3：学习和评估</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/24/cs231n-(5)神经网络-2：设置数据和Loss/">cs231n-(5)神经网络-2：设置数据和Loss</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/06/《Deep Learning》(4)-数值计算/">《Deep Learning》(4)-数值计算</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/06/《Deep Learning》(3)-概率和信息论/">《Deep Learning》(3)-概率和信息论</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/06/《Deep Learning》(2)-线性代数/">《Deep Learning》(2)-线性代数</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/06/《Deep Learning》(1)-介绍/">《Deep Learning》(1)-介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/06/cs231n-(5)神经网络-1：建立架构/">cs231n-(5)神经网络-1：建立架构</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/04/cs231n-(4)反向传播/">cs231n-(4)反向传播</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/04/cs231n-(3)最优化：随机梯度下降/">cs231n-(3)最优化：随机梯度下降</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/04/cs231n-(2)线性分类器：SVM和Softmax/">cs231n-(2)线性分类器：SVM和Softmax</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/04/cs231n-(1)图像分类和kNN/">cs231n-(1)图像分类和kNN</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/03/21/机器学习(7)-神经网络预测练习/">机器学习(7)-神经网络预测练习</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/03/15/机器学习(6)-神经网络/">机器学习(6)-神经网络</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/03/09/机器学习(5)-逻辑回归练习/">机器学习(5)-逻辑回归练习</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/03/07/机器学习(4)-正则化/">机器学习(4)-正则化</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/03/06/机器学习(3)-逻辑回归/">机器学习(3)-逻辑回归</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/02/29/机器学习(2)-线性回归/">机器学习(2)-线性回归</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/02/27/机器学习(1)-概念/">机器学习(1)-概念</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/01/18/Google-Logging-Library-glog-使用/">Google Logging Library(glog)使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/01/10/获取目录下的所有子目录和文件/">获取目录下的所有子目录和文件</a></li><li class="post-list-item"><a class="post-list-link" href="/2015/12/21/Google Protocol BUffers使用/">Google Protocol BUffers使用</a></li></ul>
    <script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
    <script>
        $(".post-list").addClass("toc-article");
        $(".post-list-item a").attr("target","_blank");
        $("#post-nav-button > a:nth-child(2)").click(function() {
            $(".fa-bars, .fa-times").toggle();
            $(".post-list").toggle(300);
            if ($(".toc").length > 0) {
                $("#toc, #tocButton").toggle(200, function() {
                    if ($(".switch-area").is(":visible")) {
                        $("#toc, .switch-btn, .switch-area").toggle();
                        $("#tocButton").attr("value", valueHide);
                        }
                    })
            }
            else {
                $(".switch-btn, .switch-area").fadeToggle(300);
            }
        })
    </script>




    <script>
        
    </script>
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                &copy; 2017 KangBing
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的静态博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减双栏 Hexo 博客主题">Yelee</a> by MOxFIVE
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" >本站到访数: 
                            <span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>, </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit">本页阅读量: 
                            <span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
    </div>
    
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js" type="text/javascript"></script>
<script src="/js/main.js" type="text/javascript"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>




    <!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>
#add by kangyabing

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>

<script>
    $(document).ready(function() {
        if ($("#comments").length < 1) {
            $("#scroll > a:nth-child(2)").hide();
        };
    })
</script>


<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>

<!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
</body>
</html>