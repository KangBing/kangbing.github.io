<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>cs231n-(5)神经网络-3：学习和评估 | KangBing&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="梯度检查，参数更新，超参数优化和模型评估">
<meta property="og:type" content="article">
<meta property="og:title" content="cs231n-(5)神经网络-3：学习和评估">
<meta property="og:url" content="http://kangbing.github.io/2016/10/22/cs231n-(5)神经网络-3：学习和评估/index.html">
<meta property="og:site_name" content="KangBing's Blog">
<meta property="og:description" content="梯度检查，参数更新，超参数优化和模型评估">
<meta property="og:image" content="https://raw.githubusercontent.com/KangBing/blog-img/master/blog/cs231n7_01.jpeg">
<meta property="og:image" content="https://raw.githubusercontent.com/KangBing/blog-img/master/blog/cs231n7_02.jpeg">
<meta property="og:image" content="https://raw.githubusercontent.com/KangBing/blog-img/master/blog/cs231n7_03.jpeg">
<meta property="og:image" content="https://raw.githubusercontent.com/KangBing/blog-img/master/blog/cs231n7_04.jpeg">
<meta property="og:image" content="https://raw.githubusercontent.com/KangBing/blog-img/master/blog/cs231n7_05..gif">
<meta property="og:image" content="https://raw.githubusercontent.com/KangBing/blog-img/master/blog/cs231n7_06..gif">
<meta property="og:updated_time" content="2018-12-19T15:27:14.378Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="cs231n-(5)神经网络-3：学习和评估">
<meta name="twitter:description" content="梯度检查，参数更新，超参数优化和模型评估">
  
    <link rel="alternative" href="/atom.xml" title="KangBing&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
      <link rel="stylesheet" href="//cdn.bootcss.com/animate.css/3.5.0/animate.min.css" type="text/css">
  
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  <link rel="stylesheet" href="/font-awesome/css/font-awesome.min.css">
  <link rel="apple-touch-icon" href="/apple-touch-icon.png">
  
  
      <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  
  <script>
      var yiliaConfig = {
          fancybox: true,
          animate: true,
          isHome: false,
          isPost: true,
          isArchive: false,
          isTag: false,
          isCategory: false,
          open_in_new: false
      }
  </script>
  
      <script>
          var _hmt = _hmt || [];
          (function() {
              var hm = document.createElement("script");
              hm.src = "//hm.baidu.com/hm.js?1ad2f92496bbe7f551683e065f125883";
              var s = document.getElementsByTagName("script")[0]; 
              s.parentNode.insertBefore(hm, s);
          })();
      </script>
  
</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            
            <img lazy-src="/img/avatar.png" class="js-avatar">
            
        </a>

        <hgroup>
          <h1 class="header-author"><a href="/">KangBing</a></h1>
        </hgroup>

        
        <p class="header-subtitle">A Championship Heart!</p>
        
                


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">分类和标签</a></li>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <li id="Email"><a class="Email" target="_blank" href="mailto:kangyabing@126.com" title="Email"></a></li>
                            
                                <li id="GitHub"><a class="GitHub" target="_blank" href="/#" title="GitHub"></a></li>
                            
                                <li id="RSS"><a class="RSS" target="_blank" href="/atom.xml" title="RSS"></a></li>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <a href="/tags/AutoDiff/" style="font-size: 10px;">AutoDiff</a> <a href="/tags/BatchNorm/" style="font-size: 10px;">BatchNorm</a> <a href="/tags/DeepLearning/" style="font-size: 20px;">DeepLearning</a> <a href="/tags/Depthwise/" style="font-size: 10px;">Depthwise</a> <a href="/tags/Ensembles/" style="font-size: 10px;">Ensembles</a> <a href="/tags/Inception/" style="font-size: 12.5px;">Inception</a> <a href="/tags/Inception-V3/" style="font-size: 10px;">Inception-V3</a> <a href="/tags/Inception-V4/" style="font-size: 10px;">Inception-V4</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/ParameterServer/" style="font-size: 10px;">ParameterServer</a> <a href="/tags/Protocol-BUffers/" style="font-size: 10px;">Protocol BUffers</a> <a href="/tags/R-CNN/" style="font-size: 10px;">R-CNN</a> <a href="/tags/Residual/" style="font-size: 12.5px;">Residual</a> <a href="/tags/SGD/" style="font-size: 10px;">SGD</a> <a href="/tags/SPP-net/" style="font-size: 10px;">SPP-net</a> <a href="/tags/Xavier/" style="font-size: 10px;">Xavier</a> <a href="/tags/Xception/" style="font-size: 10px;">Xception</a> <a href="/tags/backforward/" style="font-size: 10px;">backforward</a> <a href="/tags/c/" style="font-size: 10px;">c++</a> <a href="/tags/cs231n/" style="font-size: 17.5px;">cs231n</a> <a href="/tags/feature/" style="font-size: 10px;">feature</a> <a href="/tags/glog/" style="font-size: 10px;">glog</a> <a href="/tags/google/" style="font-size: 12.5px;">google</a> <a href="/tags/kNN/" style="font-size: 10px;">kNN</a> <a href="/tags/python/" style="font-size: 10px;">python</a> <a href="/tags/softmax/" style="font-size: 10px;">softmax</a> <a href="/tags/数值计算/" style="font-size: 10px;">数值计算</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/概率/" style="font-size: 10px;">概率</a> <a href="/tags/神经网络/" style="font-size: 15px;">神经网络</a> <a href="/tags/线性代数/" style="font-size: 10px;">线性代数</a>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a target="_blank" class="main-nav-link switch-friends-link" href="https://hexo.io">Hexo</a>
                    
                      <a target="_blank" class="main-nav-link switch-friends-link" href="https://pages.github.com/">GitHub</a>
                    
                      <a target="_blank" class="main-nav-link switch-friends-link" href="http://moxfive.xyz/">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">专注写代码</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">KangBing</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                
                    <img lazy-src="/img/avatar.png" class="js-avatar">
                
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">KangBing</a></h1>
            </hgroup>
            
            <p class="header-subtitle">A Championship Heart!</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">分类和标签</a></li>
                
                    <li><a href="/about/">关于我</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <li id="Email"><a class="Email" target="_blank" href="mailto:kangyabing@126.com" title="Email"></a></li>
                            
                                <li id="GitHub"><a class="GitHub" target="_blank" href="/#" title="GitHub"></a></li>
                            
                                <li id="RSS"><a class="RSS" target="_blank" href="/atom.xml" title="RSS"></a></li>
                            
                        </ul>
            </nav>
        </header>                
    </div>
</nav>
      <div class="body-wrap"><article id="post-cs231n-(5)神经网络-3：学习和评估" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/10/22/cs231n-(5)神经网络-3：学习和评估/" class="article-date">
      <time datetime="2016-10-22T03:06:05.000Z" itemprop="datePublished">2016-10-22</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs231n-(5)神经网络-3：学习和评估
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/cs231n笔记/">cs231n笔记</a>
    </div>


        
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DeepLearning/">DeepLearning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cs231n/">cs231n</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/神经网络/">神经网络</a></li></ul>
    </div>

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>梯度检查，参数更新，超参数优化和模型评估</p>
<ul>
<li><a id="more"></a>
</li>
</ul>
<h2 id="梯度检查">梯度检查</h2><p>梯度检查在理论上，对比推导的梯度和数值计算的梯度即可；在实践中，并不这么简单。</p>
<h3 id="使用中心化形式的梯度计算">使用中心化形式的梯度计算</h3><p>课本中都学过，梯度计算公式：</p>

$$
\frac{df(x)}{dx} = \frac{f(x + h) - f(x)}{h}
$$

<p>$h$一般是很小的值，实践中数量级为$1e-5$。</p>
<p>上面这种计算梯度公式实践中不常用，实践中常常用中心化形式：</p>

$$
\frac{df(x)}{dx} = \frac{f(x + h) - f(x - h)}{2h} 
$$

<p>这种中心化的形式，要求计算损失函数2次，带来的好处就是梯度精度提高了。可以使用泰勒级数展开验证，非中心化形式梯度计算公式误差为$O(h)$，中心化形式误差为$O(h^2)$</p>
<h3 id="使用相对误差">使用相对误差</h3><p>数值梯度$f^{‘}_n$和推导梯度$f^{‘}_a$之间差异的对比，要比较两者的相对误差：</p>

$$
\frac{\mid f'_a - f'_n \mid}{\max(\mid f'_a \mid, \mid f'_n \mid)}
$$

<p>上面的分布上的$\max$运算可以替换为加法，这样可以避免分母为零的情况，但是也要注意两者绝对值都为零的情况。在实践中：</p>
<p>1、相对误差大于1e-2时，极可能梯度计算错误。<br>2、相对误差在1e-2和1e-4之间，有可能梯度计算错误。<br>3、相对误差小于1e-4，对于不光滑的激励函数来说可以接受；对于光滑激励函数（使用tanh nonlinearities和softmax），还是过高。<br>4、小于1e-7，一般来说是正确的。</p>
<p>注意，网络结果越深，梯度误差可能越大，因为在传播时误差会积累。例如一个10层的网络，相对误差为1e-2也可以接受；但是对于一个可导函数时，1e-2就不能接受了。</p>
<h3 id="使用双精度">使用双精度</h3><p>使用双精度将更加准确，有时单精度计算相对误差为1e-2，换成双精度后会变化1e-8。</p>
<h3 id="数值范围要在单精度精确度内">数值范围要在单精度精确度内</h3><p>读一下<a href="http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html" target="_blank" rel="external">What Every Computer Scientist Should Know About Floating-Point Arithmetic</a>会让你编码更加小心，减少错误。例如神经网络中，常常要对一个batch数据计算得到的loss函数进行归一化；如果loss函数本来就很小，进行归一化再去除以一个值，得到的归一化loss函数值将会更小，从而引起一系列问题。因此，常常需要把解析梯度和数值梯度的原始值打印出来，确保它们不是特别小。如果的确比较小，可以进行放大，一般放大的数值量级为1.0。</p>
<h3 id="目标函数中的不连续点">目标函数中的不连续点</h3><p>梯度检查失败时，很可能是因为目标函数中的不连续点，函数ReLU、SVM loss、Maxout会引入不连续点。例如对于ReLU($\max(0,x)$)，$x=-1e-6$，$h&gt;-1e-6$，那么$f(x+h)$和$f(x-h)$会在不连续点的两侧。<br>这不是极端情况，对于CIFAR-10，有50000个样本，每个样本有9个目标函数，那么总共有450000个$\max(0,x)$函数。</p>
<p>在计算梯度时，可以知道是否越过了不可导点。在前向传播时，记录$\max(x,y)$那个最大，在计算$f(x+h)$和f(x-h$时，如果至少有一个最大值发生变化，那么说明越过了不可导点。</p>
<h3 id="使用少量数据点">使用少量数据点</h3><p>使用数据点少，碰到上面不可导点的情况就少。使用数据点少，可以减小计算量，增快速度；如果只是检查2-3个点的梯度，那么可以检查整个batch的梯度。</p>
<h3 id="注意h的大小">注意h的大小</h3><p>理论上$h$越小越好，但是实际中$h$不能太小，否则有数值问题，常常使用1e-4或1e-6。可以参考<a href="http://en.wikipedia.org/wiki/Numerical_differentiation" target="_blank" rel="external">这里</a>。</p>
<h3 id="在典型mode检查">在典型mode检查</h3><p>梯度检查只是在一些特定的点检查，即使这些点正确，放到全局也未必正确。尽可能选择典型mode检查梯度，例如svm训练，初始化后，不同类的score都接近0，这时最好不要检查梯度；果断事件，让网络学习一小段事件，loss函数下降之后在检查。</p>
<h3 id="不要让正则化淹没数据">不要让正则化淹没数据</h3><p>loss函数中有正则化的loss和data loss，在梯度检查时，不要让正则化的loss淹没了data loss。一种做法是在检查梯度时去掉正则化loss，检查正则化的梯度时，可以去掉data loss或增大正则化loss的权重。</p>
<h3 id="关闭drop_out和数据扩展">关闭drop out和数据扩展</h3><p>在检查梯度时，要去掉网络中的不确定因素，关掉drop out和数据扩展。关掉它们后，就无法对它们的正确性进行检查，一种更好的方法为在计算$f(x+h),f(x-h$前增加一个特定的随机种子，在计算推导梯度时也是如此。</p>
<h3 id="检查少量维度">检查少量维度</h3><p>实践中，梯度可能有几百万个参，这种情况下，只需要检查几个维度，假设其他维度正确即可。但是要注意，要检查这些维度的每一个参数。例如，偏置数量很少，随机抽取可能检查不到偏置。</p>
<h2 id="学习之前：合理检查的技巧">学习之前：合理检查的技巧</h2><h3 id="查看特定情况下的loss">查看特定情况下的loss</h3><p>当用小的值初始化参数时，确保期望的loss和实际loss一致。最好先去掉正则项，单独检查data loss。例如使用Softmax分类CIFAR-10时，初始化后得到的data loss应该是2.302（$-\ln(0.1)=2.302$）。对于SVM，初始时所有的score都为0，所以data loss为9。</p>
<h3 id="增强正则化">增强正则化</h3><p>增强正则化项时，loss增大。</p>
<h3 id="过拟合一个小数据集">过拟合一个小数据集</h3><p>在进行全量数据训练前，使用一个小数据集训练（例如20个数据），去掉正则化项，这样可以确保得到loss值为0。即使在小数据集上得到loss为0，也未必完全正确。例如数据集是随机的，对于小数据集可以拟合，但是对于大数据集还是无法拟合。</p>
<h2 id="观察训练过程">观察训练过程</h2><p>在训练过程中需要观察期过程，观察几个参数的变化过程。</p>
<p>训练过程中，日志往往记录着参数变化，可以将参数变化画成图，这样更加形象。横轴x一般表示epoch，用来衡量样本在训练中使用的次数，所有样本使用一次为1个epoch。有些使用迭代次数，需要注意的是迭代次数xbatchsize才是使用样本量。</p>
<h3 id="Loss函数">Loss函数</h3><p>第一个要观察的就是loss值的变化，它用来衡量一个batchsize的样本的前向传播。下图展示了loss值随时间变化，其形状可以得知学习率的情况：</p>
<p><img src="https://raw.githubusercontent.com/KangBing/blog-img/master/blog/cs231n7_01.jpeg" alt="cs231n7_01.jpeg"></p>
<p>左边是一个示意图，学习率过低，学习效果时线性增长。高的学习率开始时学习效果时指数增长，但最后loss值在一个较高水平（绿色线）；这是因为在优化时，“能量”过大，导致参数震荡不能找到一个最优点。右边是实际训练中loss值随时间变化图，它是在CIFAR-10上训练的。这个loss值变化看起来比较合理（lr可能比较小，也难说），batchsize可能也比较小，因为loss的噪声比较大。</p>
<p>loss值的抖动和batch size有关。batch size太小，抖动会比较厉害。如果batch是整个训练集，loss值震动就会最小，因为每次更新的梯度都是最优。</p>
<p>也有人使用对数loss函数，因为学习过程可能是指数的，取对数后学习过程会像直线。如果有交叉验证模型，把结果画到同一副图上，这样它们之间差异会比较明显。</p>
<p>有时对数loss函数看起来很搞笑:<a href="http://lossfunctions.tumblr.com/" target="_blank" rel="external">lossfunctions.tumblr.com</a></p>
<h3 id="训练/验证准确率">训练/验证准确率</h3><p>第二重要的就是观察在训练集和验证集上的准确率。这样可以直观得到模型是否过拟合。</p>
<p><img src="https://raw.githubusercontent.com/KangBing/blog-img/master/blog/cs231n7_02.jpeg" alt="cs231n7_02.jpeg"></p>
<p>训练集和验证集上准确率之间的“间隔”可以衡量过拟合情况。蓝色线的验证集准确率和训练集准确率间隔过大，说明已经过拟合（验证集准确率可能在增长后还会下降）。在实践中遇到这种情况，一是加强正则化（更强的L2,更多的drop out），二是增大训练集。绿色线情况，验证集和训练集准确率变化完全一致，说明模型容量还不够大，增加参数来增加模型容量。</p>
<h3 id="权重和更新的比例">权重和更新的比例</h3><p>最后要观察的是更新幅度和参数幅度之间的比值。注意，这里的更新不是原始梯度，例如在sgd中，是梯度乘以学习率。需要对每个参数及单独计算。这个比例的一个启发值是大概1e-3左右。如果太低，学习率太小，如果高学习率过大。下面是计算过程</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># assume parameter vector W and its gradient vector dW</span></span><br><span class="line">param_scale = np.linalg.norm(W.ravel())</span><br><span class="line">update = -learning_rate*dW <span class="comment"># simple SGD update</span></span><br><span class="line">update_scale = np.linalg.norm(update.ravel())</span><br><span class="line">W += update <span class="comment"># the actual update</span></span><br><span class="line"><span class="keyword">print</span> update_scale / param_scale <span class="comment"># want ~1e-3</span></span><br></pre></td></tr></table></figure>
<p>不是计算最大或最小值，而是计算和观察其范数的值。这些矩阵通常是相关的，也能得到近似的结果。</p>
<h3 id="激活/每层梯度分布">激活/每层梯度分布</h3><p>参数的错误初始化会减慢学习速度，甚至使学习停止。这个问题比较容易解决，一种解决方法是画图每层梯度/激活函数的直方图。直觉上看，如果有奇怪的分布，那么可能就有问题。例如tanh神经元输出的直方图分布，应该可以看到参数在[-1,1]之间所有数值都有分布，但是如果全是0或者-1、+1，那么一般就是有问题了。</p>
<p>第一层可视化<br>如果使用像素数据，那么可视化第一层可能会有帮助：<br><img src="https://raw.githubusercontent.com/KangBing/blog-img/master/blog/cs231n7_03.jpeg" alt="cs231n7_03.jpeg"></p>
<p>左边的图中有许多噪声，网络没有收敛，可能是学习率或正则化的问题。右边的图比较平滑、干净、特征比较多，说明学习过程很好。</p>
<h2 id="参数更新">参数更新</h2><p>在反向传播中计算好解析梯度，使用解析梯度更新参数，有不同的方法来更新参数。</p>
<p>神经网络的优化是当前非常热额研究热点。本文只是讲解实践中常常用到的技巧，给出直观上的解释。如果需要了解细节，我们提供了额外读物。</p>
<h3 id="SGD_and_bells_and_whistles">SGD and bells and whistles</h3><p><strong>Vanilla update</strong><br>最简单的更新就是沿着梯度负方向更新。夹着<code>x</code>是参数向量，<code>dx</code>是梯度向量<br><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Vanilla update</span></span><br><span class="line">x += - learning_rate <span class="keyword">*</span> dx</span><br></pre></td></tr></table></figure></p>
<p>这里<code>learning_rate</code>是一个超参数，它是固定值。当使用这个训练集，且学习率足够小，就可以降低loss函数值。</p>
<h3 id="Momentum_update">Momentum update</h3><p>Momentum更新在深度网络中总能有更好的收敛速度。这种更新方式可以从物理优化角度来解释。loss值看一看到山的高度（有高度就有势能$U=mgh$， 且$U \propto h$）。用随机数初始化参数等价于设置质点在某位置的速度为0；优化过程可以看做参数向量（即质点）滚动的过程。</p>
<p>因为质点所受的力和势能的梯度相关（$F = - \nabla U$），质点所受的力是loss函数的负梯度。有$F=ma$，所以负梯度和质点的加速度成正比。这个观点和SGD不同，SGD是直接合并位置；而物理学观点是梯度只是影响速度，速度影响位置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Momentum update</span></span><br><span class="line">v = mu * v - learning_rate * dx <span class="comment"># integrate velocity</span></span><br><span class="line">x += v <span class="comment"># integrate position</span></span><br></pre></td></tr></table></figure>
<p>这里引入了参数<code>v</code>，初始化为0，还引入了参数<code>mu</code>。说的不恰当一些，这个参数可以当做动量(momentum，常常设置为0.9），它的物理意义是和摩擦力一致。这个变量减小了速度，降低了系统的势能，否则质点在山底也不会停下来。使用交叉验证时，这个参数可以设置为$[0.5, 0.9. 0.95. 0.95]$。和学习率退火时间类似，动量随时间设置不同值可以略微改善优化效果，动量在学习后阶段可以增大。一个典型的设置是在初始时设置为0.5，后面随着epoch增大，直至到0.99.</p>
<p>随着动量更新，参数会在有梯度的方向上增加速度。</p>
<h3 id="Nesterov_Momentum">Nesterov Momentum</h3><p>Nesterov Momentum和标准的Momentum有点不同，对于凸优化，在理论上它能有更好的收敛，在实践中，它的性能优于标准momentum。</p>
<p>Nesterov Momentum的核心思想是，当参数向量在位置<code>x</code>时，回看一下momentum更新，只考虑动量部分（忽略第二项梯度部分），通过<code>mv*v</code>影响参数。当我们打算计算梯度梯度时，把未来一个近似位置<code>x+mu*v</code>当做“向前看”-这个点是将来更新后位置的附近，计算这个点<code>x+mu*v</code>的梯度，而不是旧位置<code>x</code>。</p>
<p><img src="https://raw.githubusercontent.com/KangBing/blog-img/master/blog/cs231n7_04.jpeg" alt="cs231n7_04.jpeg"></p>
<p>左边是标准的Momentum update，右边是Nesterov momentum update。红色的点是“旧位置”。</p>
<p>实现如下:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_ahead = x + mu * v</span><br><span class="line"><span class="comment"># evaluate dx_ahead (the gradient at x_ahead instead of at x)</span></span><br><span class="line">v = mu * v - learning_rate * dx_ahead</span><br><span class="line">x += v</span><br></pre></td></tr></table></figure></p>
<p>在实践中，人们更喜欢类似标准SGD或momentum update的形式。可以通过参数<code>x_ahead=x+m*v</code>改写实现，用<code>x_ahead</code>表示更新，不用<code>x</code>。即实际中计算时总是计算前一步的版本。公式中<code>x_ahead</code>（改回<code>x</code>）变为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">v_prev = v <span class="comment"># back this up</span></span><br><span class="line">v = mu * v - learning_rate * dx <span class="comment"># velocity update stays the same</span></span><br><span class="line">x += -mu * v_prev + (<span class="number">1</span> + mu) * v <span class="comment"># position update changes form</span></span><br></pre></td></tr></table></figure></p>
<p>我们推荐阅读公式来源和数学推导Nesterov’s Accelerated Momentum(NAG):</p>
<p><a href="http://arxiv.org/pdf/1212.0901v2.pdf" target="_blank" rel="external">Advances in optimizing Recurrent Networks</a> by Yoshua Bengio, Section 3.5.<br><a href="http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf" target="_blank" rel="external">Ilya Sutskever’s thesis</a> (pdf) contains a longer exposition of the topic in section 7.2</p>
<h3 id="Annealing_the_learning_rate">Annealing the learning rate</h3><p>在训练深度网络过程中，要学会对学习率退火。如果学习率太高，系统就有太大的动能，参数在某一范围内波动，loss函数不能到达更窄更深的位置。合适减小学习率是有技巧的，如果减小太慢，loss函数值会一直某一范围波动，提高很少，这样浪费计算资源；如果减小太快，系统动能很快降了下来，将不能到达最好的位置。常常用到的退火策略有以下三种：</p>
<p><strong>Step decay</strong><br>经过几个epoch，就降低学习率。典型值为讲过5个epoch就减小为一半，或经过20个epochs变为原来的0.1倍。当然这些具体值要依具体问题和模型而定。一个启发式的做法是在实践训练中观察交叉验证集的错误率，入股错误率停止增大，学习率乘以一个常量（例如0.5）。</p>
<p><strong>Exponential decay</strong><br>数学形式为$\alpha = \alpha_0 e^{-k t}$，其中$\alpha_0,k$是超参数，$t$是迭代。</p>
<p><strong>1/t decay</strong><br>数学形式为$\alpha = \alpha_0 / (1 + k t )$，其中$\alpha_0,k$是超参数，$t$是迭代。</p>
<h3 id="Second_order_methods">Second order methods</h3><p>还有一类优化的方法，主要是基于<a href="http://en.wikipedia.org/wiki/Newton%27s_method_in_optimization" target="_blank" rel="external">Newton’s method</a>，它的迭代形式为:</p>

$$
x \leftarrow x - [H f(x)]^{-1} \nabla f(x)
$$

<p>其中$H f(x)$是<a href="http://en.wikipedia.org/wiki/Hessian_matrix" target="_blank" rel="external">Hessian matrix</a>，它是函数二阶偏导数组成的方阵。$\nabla f(x)$是梯度向量。直观上看，Hessian matrix描述了loss函数的局部曲率，局部曲率可以更我们更高效的更新参数。乘以Hessian matrix的转置，在曲率小时大步前进，在曲率大时小步前进。注意，上面更新中没有学习率，这正是比一阶导数有优势的地方。</p>
<p>二阶导数虽然理论上更加有效，但是实际中很少用到。因为矩阵求逆计算量太大，且占用太多内存。对于Hessian矩阵求逆也有一些研究实际中有一些计算近似的方法，例如<a href="http://en.wikipedia.org/wiki/Limited-memory_BFGS" target="_blank" rel="external">L-BFGS</a>，具体内容可以参考</p>
<p><a href="http://research.google.com/archive/large_deep_networks_nips2012.html" target="_blank" rel="external">Large Scale Distributed Deep Networks</a> is a paper from the Google Brain team, comparing L-BFGS and SGD variants in large-scale distributed optimization.<br><a href="http://arxiv.org/abs/1311.2115" target="_blank" rel="external">SFO</a> algorithm strives to combine the advantages of SGD with advantages of L-BFGS.</p>
<h3 id="Per-parameter_adaptive_learning_rate_methods">Per-parameter adaptive learning rate methods</h3><p>前面的所有方法都是对所有参数同时使用。调优学习率是昂贵的过程，因此许多工作投入到自适应学习率，甚至每个参数适应学习率。虽然很多方法需要设置其他超参数，但是这些超参数在比较大的变化范围上，不会太影响系统性能，但是原始学习率会影响。下面，介绍一些在实践中常常用到的自适应算法。</p>
<p><strong>Adagrad</strong><br>Adagrad是由<a href="http://jmlr.org/papers/v12/duchi11a.html" target="_blank" rel="external">Duchi et al..</a>提出，实现如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Assume the gradient dx and parameter vector x</span></span><br><span class="line">cache += dx**<span class="number">2</span></span><br><span class="line">x += - learning_rate * dx / (np.sqrt(cache) + eps)</span><br></pre></td></tr></table></figure>
<p><code>cache</code>和<code>dx</code>大小相同，跟踪每个参数的平方和，用来归一化参数更新，需要注意它是element-wise。从代码中可以看出，如果梯度大，那么将会减小学习率；如果梯度小，将会增大学习率。求平方根操作非常重要，没有它算法效果将会非常差。平滑项<code>eps</code>（范围通常为1e-4到1e-8）可以避免分母为零。Adagrad的缺点是在深度学习中，单调的学习率被证明通常是太激进，很早就使得学习停止了。</p>
<p><strong>RMSprop</strong><br>RMSprop是非常高效但是没有公开发表的自适应学习率的方法。它是Adagrad方法的修改版，减小它的集锦性，单调减小学习率。尤其是它使用了梯度平方的滑动平均。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cache = decay_rate * cache + (<span class="number">1</span> - decay_rate) * dx**<span class="number">2</span></span><br><span class="line">x += - learning_rate * dx / (np.sqrt(cache) + eps)</span><br></pre></td></tr></table></figure>
<p>其中<code>decay_rate</code>是超参数，其值一般为[0.9, 0.99, 0.999]。<code>x+=</code>更新部分和Adagrad相同，但是<code>cache</code>变量部分不同。RMSProp依然是基于梯度大小对学习率进行调节，有不错的效果，和Adagrad不同的是，它不会单调递减学习率。</p>
<p><strong>Adam</strong><br><a href="http://arxiv.org/abs/1412.6980" target="_blank" rel="external">Adam</a>是最近提出的，有点像带有动量的RMSProp，它的精简版更新如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m = beta1*m + (<span class="number">1</span>-beta1)*dx</span><br><span class="line">v = beta2*v + (<span class="number">1</span>-beta2)*(dx**<span class="number">2</span>)</span><br><span class="line">x += - learning_rate * m / (np.sqrt(v) + eps)</span><br></pre></td></tr></table></figure>
<p>注意到其更新非常像RMSProp，不同点是这里使用的是平滑版本的梯度<code>m</code>，而不是原始梯度<code>dx</code>。论文中推荐参数的值<code>eps=1e-8</code>,<code>beta1</code>=0.9<code>,&#39;beta2=0.999</code>。实践中，推荐默认使用Adam，它的效果往往好于RMSProp；但是也推荐尝试一下SGD+Nesterov Momentum。完整的Adam更新包括偏置矫正机制，因为向量<code>m,v</code>在前几步中初始化，因此网络“热身”之前，偏置为零。了解更多细节，参考论文或课外读物。</p>
<p>课外读物：<br><a href="http://arxiv.org/abs/1312.6055" target="_blank" rel="external">Unit Tests for Stochastic Optimization</a> proposes a series of tests as a standardized benchmark for stochastic optimization.</p>
<p>通过下面两张动画可以直观感受不同优化算法：<br><img src="https://raw.githubusercontent.com/KangBing/blog-img/master/blog/cs231n7_05..gif" alt="cs231n7_05.gif"><br><img src="https://raw.githubusercontent.com/KangBing/blog-img/master/blog/cs231n7_06..gif" alt="cs231n7_06..gif"></p>
<p>上面图片是loss函数的等高线。注意到带有动量的方法，一旦走偏，很难纠正回来，它的线路就行小球向下滚。下面图片是带有鞍点的图形，某些维度开口向上，某位维度开口向下。注意到SGD方法很难突破对称性的限制，被困在了高点。与此相反的是，像RMSprop这样的算法可以看到鞍点更低的梯度；因为更新过程中分母选项的原因，将会增大这个方向的学习率，帮助RMSProp在这个方向前进。</p>
<h2 id="超参数优化">超参数优化</h2><p>训练神经网络包括许多超参数集，神经网络中最常见的包括：<br>1、初始化学习率<br>2、学习率衰减算法（例如常量衰减）<br>3、正则化强度（L2，dropout 强度）</p>
<p>还有一些相对弱一些的超参数（神经网络不那么敏感），例如per-parameter adaptive learning method， 动量及其设置。这一小节，讲解调参的额外方法和技巧。</p>
<h3 id="实现">实现</h3><p>大的神经网络需要很长时间来训练，调参可能需要几天甚至几周。设置代码时，要有一个worker连续随机设置参数，之后优化。在训练时worker在完成一个epoch后，测试在验证集上的性能，且保存check point到文件，文件中要有在验证集上性能的信息。还要有一个master程序，用来管理worker程序，还可以检查check point，记录训练过程等。</p>
<h3 id="优先选用一个验证集，而不是交叉验证">优先选用一个验证集，而不是交叉验证</h3><p>在大多场景中，使用大小合适的验证集可以让代码简单，没必要使用多折交叉验证。可能常常听说用交叉验证集来设置超参数，在大多情况下，只是使用了一个验证集。</p>
<h3 id="超参数的范围">超参数的范围</h3><p>以对数为尺度来设置超参数。例如，学习率取样范围<code>learning_rate = 10**uniform(-6,1)</code>；即学习率是10的指数幂，幂的大小(-6,1)上的随机数。这个策略也适用于正则化强度。这是因为学习率和正则化强度是乘的影响，因为对于它们值的范围取值，乘以或除以某个数，而不是加上或减去某个数。但是有些超参数（drop out）是在原有尺度进行设置，<code>dropout=unifor(0,1)</code>。</p>
<h3 id="优先使用随机搜索，而不是网格搜索">优先使用随机搜索，而不是网格搜索</h3><p>论证过程参考<a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" target="_blank" rel="external"> Random Search for Hyper-Parameter Optimization</a>。原理大概为，超参数中，一些超参数比另一些重要，因此随机搜索更有可能找到更优的超参数。</p>
<h3 id="对边界上的最优值格外小心">对边界上的最优值格外小心</h3><p>边界上的最优值可能不是全局最优。例如，对于学习率，其范围<code>learning_rate = 10 ** uniform(-6, 1)</code>，最终使用的学习率不应该出现在边界处，如果时，要扩大范围来进一步搜索。</p>
<h3 id="由粗到细来搜索">由粗到细来搜索</h3><p>实践中，现在一个大范围搜索，之后逐步缩小搜索范围。在大范围搜索时，只需要一个或半个epoch即可，因为某些不合适的超参数极有可能让网络学不到任何东西，甚至引起loss值爆炸。在细搜时，让训练运行5个左右的epoch，缩小范围后，还可以进一步细搜，直到找到最优值。</p>
<h3 id="贝叶斯超参数优化">贝叶斯超参数优化</h3><p>贝叶斯超参数优化是专注于超参数空间更高效导航算法的研究领域。其核心思想是在探索上找平衡–探索和不同超参数设置之间的平衡。在此基础上开发出一些库。但是在卷积网络实践中，找一个范围认真设置的性能一般很难超越随机搜索。<a href="http://nlpers.blogspot.com/2014/10/hyperparameter-search-bayesian.html" target="_blank" rel="external">这里</a>有更详细的讨论。</p>
<h2 id="评估">评估</h2><h3 id="模集成">模集成</h3><p>在实践中，可以把性能提升几个百分点的一种方法是：训练多个独立的模型，在测试时把多个模型的结果求平均。随着模型数量增大，性能会单调递增，但是越到后面提升越少。不同模型越是多样性，性能提升越是明显。有以下几种方式集成：</p>
<p><strong>同一个模型，不同初始化</strong><br>使用交叉验证找到最优的超参数。使用最优超参数、不同初始化方式来训练。这种方式缺点是它仅仅依靠不同初始化。</p>
<p><strong>交叉验证选择最优模型</strong><br>使用交叉验证找到最优模型，选择最优参数的前几个（例如10个）来做模型集成，增加模型的多样性。这样可能会把次优模型包含进来。这种方式比较容易实践，因为它在交叉验证后不需要再训练了。</p>
<p><strong>同一个模型，不同记录点</strong><br>训练过程中，随着时间进行，每个epoch都保存一个模型，这样就会有多个模型，使用这些模型来集成。这样虽然缺少多样性，但是在实践中也有良好性能。优点是这种方式代价很小。</p>
<p><strong>训练时运行参数的平均值</strong><br>和上一个方法相关，一种代价很小，但是可以提升模型一两个百分点的方法：训练时在内存保存参数的拷贝，当loss函数出现指数下降时，记录这个参数；最终使用记录这些参数的平均。这个平滑版本的参数在验证集上有更小的误差。一个直观理解为目标函数是碗状的，参数在碗的周边跳跃，参数的平均更有可能到达碗的更深处。</p>
<p>模型集成的缺点就是测试单个样本时，计算量增大了。Geoff Hinton的论文<a href="https://www.youtube.com/watch?v=EK61htlw8hY" target="_blank" rel="external">Dark Konwledge</a>提出集成模型为单个模型的方法，通过修改目标函数，加入似然函数来实现。</p>
<h2 id="总结">总结</h2><p>训练一个神经网络<br>1、梯度检查，使用小批量数据。<br>2、合理性检查，确保初始化的loss合理，过拟合小数据集，在小的训练集上达到100%准确率。<br>3、训练时，监督loss，训练集/验证集准确率。参数更新幅度和参数值比值应该在1e-3附近；如果训练神经网络，可视化第一层权重。<br>4、推荐使用的2种更新方法：SGD+Nesterov Momentum or Adam。<br>5、训练不同阶段，减小学习率。例如，固定几个epoch后降低或验证集识别率停止后降低。<br>6、随机搜索超参数（不是网格搜索），分阶段搜索（1-5个epoch在大的范围搜索，之后搜索范围）。<br>7、使用模型集成得到额外提升。</p>
<h2 id="拓展参考">拓展参考</h2><p><a href="http://research.microsoft.com/pubs/192769/tricks-2012.pdf" target="_blank" rel="external">SGD</a> tips and tricks from Leon Bottou<br><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" target="_blank" rel="external">Efficient BackProp</a> (pdf) from Yann LeCun<br><a href="http://arxiv.org/pdf/1206.5533v2.pdf" target="_blank" rel="external">Practical Recommendations for Gradient-Based Training of Deep Architectures</a> from Yoshua Bengio</p>

      
    </div>
    
  </div>
  
    
    <div class="copyright">
        <p><span>本文标题:</span><a href="/2016/10/22/cs231n-(5)神经网络-3：学习和评估/">cs231n-(5)神经网络-3：学习和评估</a></p>
        <p><span>文章作者:</span><a href="/" title="访问 KangBing 的个人博客">KangBing</a></p>
        <p><span>发布时间:</span>2016年10月22日 - 11时06分</p>
        <p><span>最后更新:</span>2018年12月19日 - 23时27分</p>
        <p>
            <span>原始链接:</span><a class="post-url" href="/2016/10/22/cs231n-(5)神经网络-3：学习和评估/" title="cs231n-(5)神经网络-3：学习和评估">http://kangbing.github.io/2016/10/22/cs231n-(5)神经网络-3：学习和评估/</a>
            <span class="copy-path" data-clipboard-text="原文: http://kangbing.github.io/2016/10/22/cs231n-(5)神经网络-3：学习和评估/　　作者: KangBing" title="点击复制文章链接"><i class="fa fa-clipboard"></i></span>
            <script src="/js/clipboard.min.js"></script>
            <script> var clipboard = new Clipboard('.copy-path'); </script>
        </p>
        <p>
            <span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" title="中国大陆 (CC BY-NC-SA 3.0 CN)" target = "_blank">"署名-非商用-相同方式共享 3.0"</a> 转载请保留原文链接及作者。
        </p>
    </div>



    <nav id="article-nav">
        
            <div id="article-nav-newer" class="article-nav-title">
                <a href="/2016/12/03/Python实现三层神经网络/">
                    Python实现三层神经网络
                </a>
            </div>
        
        
            <div id="article-nav-older" class="article-nav-title">
                <a href="/2016/09/24/cs231n-(5)神经网络-2：设置数据和Loss/">
                    cs231n-(5)神经网络-2：设置数据和Loss
                </a>
            </div>
        
    </nav>

  
</article>

    <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度检查"><span class="toc-number">1.</span> <span class="toc-text">梯度检查</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#使用中心化形式的梯度计算"><span class="toc-number">1.1.</span> <span class="toc-text">使用中心化形式的梯度计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用相对误差"><span class="toc-number">1.2.</span> <span class="toc-text">使用相对误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用双精度"><span class="toc-number">1.3.</span> <span class="toc-text">使用双精度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数值范围要在单精度精确度内"><span class="toc-number">1.4.</span> <span class="toc-text">数值范围要在单精度精确度内</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#目标函数中的不连续点"><span class="toc-number">1.5.</span> <span class="toc-text">目标函数中的不连续点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用少量数据点"><span class="toc-number">1.6.</span> <span class="toc-text">使用少量数据点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#注意h的大小"><span class="toc-number">1.7.</span> <span class="toc-text">注意h的大小</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#在典型mode检查"><span class="toc-number">1.8.</span> <span class="toc-text">在典型mode检查</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#不要让正则化淹没数据"><span class="toc-number">1.9.</span> <span class="toc-text">不要让正则化淹没数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#关闭drop_out和数据扩展"><span class="toc-number">1.10.</span> <span class="toc-text">关闭drop out和数据扩展</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#检查少量维度"><span class="toc-number">1.11.</span> <span class="toc-text">检查少量维度</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#学习之前：合理检查的技巧"><span class="toc-number">2.</span> <span class="toc-text">学习之前：合理检查的技巧</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#查看特定情况下的loss"><span class="toc-number">2.1.</span> <span class="toc-text">查看特定情况下的loss</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#增强正则化"><span class="toc-number">2.2.</span> <span class="toc-text">增强正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#过拟合一个小数据集"><span class="toc-number">2.3.</span> <span class="toc-text">过拟合一个小数据集</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#观察训练过程"><span class="toc-number">3.</span> <span class="toc-text">观察训练过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Loss函数"><span class="toc-number">3.1.</span> <span class="toc-text">Loss函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#训练/验证准确率"><span class="toc-number">3.2.</span> <span class="toc-text">训练/验证准确率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#权重和更新的比例"><span class="toc-number">3.3.</span> <span class="toc-text">权重和更新的比例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#激活/每层梯度分布"><span class="toc-number">3.4.</span> <span class="toc-text">激活/每层梯度分布</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参数更新"><span class="toc-number">4.</span> <span class="toc-text">参数更新</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#SGD_and_bells_and_whistles"><span class="toc-number">4.1.</span> <span class="toc-text">SGD and bells and whistles</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Momentum_update"><span class="toc-number">4.2.</span> <span class="toc-text">Momentum update</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Nesterov_Momentum"><span class="toc-number">4.3.</span> <span class="toc-text">Nesterov Momentum</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Annealing_the_learning_rate"><span class="toc-number">4.4.</span> <span class="toc-text">Annealing the learning rate</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Second_order_methods"><span class="toc-number">4.5.</span> <span class="toc-text">Second order methods</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Per-parameter_adaptive_learning_rate_methods"><span class="toc-number">4.6.</span> <span class="toc-text">Per-parameter adaptive learning rate methods</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#超参数优化"><span class="toc-number">5.</span> <span class="toc-text">超参数优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#实现"><span class="toc-number">5.1.</span> <span class="toc-text">实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#优先选用一个验证集，而不是交叉验证"><span class="toc-number">5.2.</span> <span class="toc-text">优先选用一个验证集，而不是交叉验证</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#超参数的范围"><span class="toc-number">5.3.</span> <span class="toc-text">超参数的范围</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#优先使用随机搜索，而不是网格搜索"><span class="toc-number">5.4.</span> <span class="toc-text">优先使用随机搜索，而不是网格搜索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#对边界上的最优值格外小心"><span class="toc-number">5.5.</span> <span class="toc-text">对边界上的最优值格外小心</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#由粗到细来搜索"><span class="toc-number">5.6.</span> <span class="toc-text">由粗到细来搜索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#贝叶斯超参数优化"><span class="toc-number">5.7.</span> <span class="toc-text">贝叶斯超参数优化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#评估"><span class="toc-number">6.</span> <span class="toc-text">评估</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#模集成"><span class="toc-number">6.1.</span> <span class="toc-text">模集成</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">7.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#拓展参考"><span class="toc-number">8.</span> <span class="toc-text">拓展参考</span></a></li></ol>
</div>
<style>
    .left-col .switch-btn {
        display: none;
    }
    .left-col .switch-area {
        display: none;
    }
</style>

<input type="button" id="tocButton" value="隐藏目录"  title="点击按钮隐藏或者显示文章目录">

<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js" type="text/javascript"></script>
<script>
    var valueHide = "隐藏目录";
    var valueShow = "显示目录";

    if ($(".left-col").is(":hidden")) {
        $("#tocButton").attr("value", valueShow);
    }

    $("#tocButton").click(function() {
        if ($("#toc").is(":hidden")) {
            $("#tocButton").attr("value", valueHide);
            $("#toc").slideDown(320);
            $(".switch-btn, .switch-area").fadeOut(300);
        }
        else {
            $("#tocButton").attr("value", valueShow);
            $("#toc").slideUp(350);
            $(".switch-btn, .switch-area").fadeIn(500);
        }
    })

    if ($(".toc").length < 1) {
        $("#toc, #tocButton").hide();
        $(".switch-btn, .switch-area").show();
    }
</script>




    <div class="share">
    <div class="bdsharebuttonbox">
    <a href="#" class="bds_more" data-cmd="more"></a>
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
    <a href="#" class="bds_sqq" data-cmd="sqq" title="分享给 QQ 好友"></a>
    <a href="#" class="bds_copy" data-cmd="copy" title="复制网址"></a>
    <a href="#" class="bds_mail" data-cmd="mail" title="通过邮件分享"></a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="生成文章二维码"></a>
    </div>
    <script>
        window._bd_share_config={
            "common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
    </script>
</div>



    
        <section class="youyan" id="comments">
  <div id="uyan_frame"></div>
  <script src="http://v2.uyan.cc/code/uyan.js?uid=2136539"></script>
</section>

    




    <div class="scroll" id="post-nav-button">
        
            <a href="/2016/12/03/Python实现三层神经网络/" title="上一篇: Python实现三层神经网络">
                <i class="fa fa-angle-left"></i>
            </a>
        

        <a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a>

        
            <a href="/2016/09/24/cs231n-(5)神经网络-2：设置数据和Loss/" title="下一篇: cs231n-(5)神经网络-2：设置数据和Loss">
                <i class="fa fa-angle-right"></i>
            </a>
        
    </div>

    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/11/04/MXNet/自动微分/">自动微分</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/25/MXNet/MXNet Data IO/">MXNet Data IO</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/14/MXNet/PS-Lite源码分析/">PS-Lite源码分析</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/26/Paper笔记/00010_Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition/">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/24/Paper笔记/0009_Rich feature hierarchies for accurate object detection and semantic segmentation/">Rich feature hierarchies for accurate object detection and semantic segmentation</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/13/Paper笔记/0008_A Neural Algorithm of Artistic Style/">A Neural Algorithm of Artistic Style</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/09/Paper笔记/0007_Xception Deep Learning with Depthwise Separable Convolutios/">Xception--Deep Learning with Depthwise Separable Convolutios</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/06/Paper笔记/0006_Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning/">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/04/Paper笔记/0005_Rethinking the Inception Architecture for Computer Vision/">Rethinking the Inception Architecture for Computer Vision</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/31/Paper笔记/0004_Residual Networks Behave Like Ensembles of Relatively Shallow Networks/">Residual Networks Behave Like Ensembles of Relatively Shallow Networks</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/14/Paper笔记/0003_Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift/">Batch Normalization--Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/11/Paper笔记/0002_Understanding the difficulty of training deep feedforward neural networks/">Understanding the difficulty of training deep feedforward neural networks</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/22/Paper笔记/0001_ILSVRC历届冠军论文笔记/">ILSVRC历届冠军论文笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/18/cs231n-(9)迁移学习和Fine-tune网络/">cs231n-(9)迁移学习和Fine-tune网络</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/18/cs231n-(8)理解和可视化卷积网络/">cs231n-(8)理解和可视化卷积网络</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/18/cs231n-(7)卷积神经网络：架构，卷积层,池化层/">cs231n-(7)卷积神经网络：架构，卷积层/池化层</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/18/cs231n-(6)实现Minimal神经网络/">cs231n-(6)实现Minimal神经网络</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/03/Python实现三层神经网络/">Python实现三层神经网络</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/22/cs231n-(5)神经网络-3：学习和评估/">cs231n-(5)神经网络-3：学习和评估</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/24/cs231n-(5)神经网络-2：设置数据和Loss/">cs231n-(5)神经网络-2：设置数据和Loss</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/06/《Deep Learning》(4)-数值计算/">《Deep Learning》(4)-数值计算</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/06/《Deep Learning》(3)-概率和信息论/">《Deep Learning》(3)-概率和信息论</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/06/《Deep Learning》(2)-线性代数/">《Deep Learning》(2)-线性代数</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/06/《Deep Learning》(1)-介绍/">《Deep Learning》(1)-介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/06/cs231n-(5)神经网络-1：建立架构/">cs231n-(5)神经网络-1：建立架构</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/04/cs231n-(4)反向传播/">cs231n-(4)反向传播</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/04/cs231n-(3)最优化：随机梯度下降/">cs231n-(3)最优化：随机梯度下降</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/04/cs231n-(2)线性分类器：SVM和Softmax/">cs231n-(2)线性分类器：SVM和Softmax</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/04/cs231n-(1)图像分类和kNN/">cs231n-(1)图像分类和kNN</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/03/21/机器学习(7)-神经网络预测练习/">机器学习(7)-神经网络预测练习</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/03/15/机器学习(6)-神经网络/">机器学习(6)-神经网络</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/03/09/机器学习(5)-逻辑回归练习/">机器学习(5)-逻辑回归练习</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/03/07/机器学习(4)-正则化/">机器学习(4)-正则化</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/03/06/机器学习(3)-逻辑回归/">机器学习(3)-逻辑回归</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/02/29/机器学习(2)-线性回归/">机器学习(2)-线性回归</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/02/27/机器学习(1)-概念/">机器学习(1)-概念</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/01/18/Google-Logging-Library-glog-使用/">Google Logging Library(glog)使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/01/10/获取目录下的所有子目录和文件/">获取目录下的所有子目录和文件</a></li><li class="post-list-item"><a class="post-list-link" href="/2015/12/21/Google Protocol BUffers使用/">Google Protocol BUffers使用</a></li></ul>
    <script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
    <script>
        $(".post-list").addClass("toc-article");
        $(".post-list-item a").attr("target","_blank");
        $("#post-nav-button > a:nth-child(2)").click(function() {
            $(".fa-bars, .fa-times").toggle();
            $(".post-list").toggle(300);
            if ($(".toc").length > 0) {
                $("#toc, #tocButton").toggle(200, function() {
                    if ($(".switch-area").is(":visible")) {
                        $("#toc, .switch-btn, .switch-area").toggle();
                        $("#tocButton").attr("value", valueHide);
                        }
                    })
            }
            else {
                $(".switch-btn, .switch-area").fadeToggle(300);
            }
        })
    </script>




    <script>
        
    </script>
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                &copy; 2018 KangBing
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的静态博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减双栏 Hexo 博客主题">Yelee</a> by MOxFIVE
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" >本站到访数: 
                            <span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>, </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit">本页阅读量: 
                            <span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
    </div>
    
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js" type="text/javascript"></script>
<script src="/js/main.js" type="text/javascript"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>




    <!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>
#add by kangyabing

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>

<script>
    $(document).ready(function() {
        if ($("#comments").length < 1) {
            $("#scroll > a:nth-child(2)").hide();
        };
    })
</script>


<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>

<!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
</body>
</html>